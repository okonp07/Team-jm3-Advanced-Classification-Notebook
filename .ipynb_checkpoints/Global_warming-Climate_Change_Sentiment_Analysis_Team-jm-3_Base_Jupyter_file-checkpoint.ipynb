{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Climate Change Belief Analysis 2022 \n",
    "\n",
    "## By Datafluent Inc. (Team JM_3)\n",
    "\n",
    "\n",
    "<div align=\"center\" style=\"width: 500px; font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://www.itu.int/en/mediacentre/backgrounders/PublishingImages/climate-change-backgrounder.jpg\"\n",
    "     alt=\"Dummy image 1\"\n",
    "     style=\"float: center; padding-bottom=0.5em\"\n",
    "     width=500px/>\n",
    "    </div>\n",
    "\n",
    "## Board Members\n",
    "1. **Prince Okon** *(CEO)*\n",
    "2. **Marvic Cocouvi** *(Director Marketing and Promotions)*\n",
    "3. **Abiemwense Omokaro** *(Director IT/Technical Support)*\n",
    "4. **Buhari Shehu** *(Wrangler General)*\n",
    "5. **Nqosa Lehloenya** *(Director Business Management)*\n",
    "6. **Kefa Kiprono**\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "\n",
    "## Outline\n",
    "- Introduction\n",
    "- Exploratory Data Analysis\n",
    "- Model Building\n",
    "- Conclusion and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Introduction\n",
    "\n",
    "## 1.1 Project Overview\n",
    "Industrialization is the enabler of modern economic growth and development. However, this comes at the cost of emitting greenhouse gases that contribute, negatively, to climate change and ultimately global warming. Governments, eco-conscious organizations, and civil societies around the world are constantly exploring ways to reduce their carbon footprints. Despite many indices pointing towards climate change, many people are of the belief that climate change is a hoax. In this project, we are going to use the novel tweets of some individuals to build a Machine Learning (ML) model to identify their beliefs about climate change. This modelâ€™s outcome will help companies to predict how their eco-friendly products will be received by their prospective customers and thus enable them to make strategic business decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Installing Dependencies and Importing Packages\n",
    "In order to successfully build the models, there is a need to pip install some dependencies. Thus, we install the *autotime library, Comet, imblearn, and nltk.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 33.8 s (started: 2022-06-21 11:39:12 +01:00)\n"
     ]
    }
   ],
   "source": [
    "# installations\n",
    "%%capture\n",
    "!pip install ipython-autotime\n",
    "!pip install comet-ml\n",
    "!pip install imblearn --user\n",
    "!pip install --user -U nltk\n",
    "!pip install wordcloud\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install -c anaconda -c conda-forge -c comet_ml comet_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2022-06-21 11:56:10 +01:00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Buhari\n",
      "[nltk_data]     Shehu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Buhari\n",
      "[nltk_data]     Shehu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Libraries for data loading, data manipulation and data visulisation\n",
    "import numpy as np # for linear algebra\n",
    "import pandas as pd # for importing, creating and manipulating dataframes\n",
    "\n",
    "# Visualization Packages\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Warnings\n",
    "import warnings \n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Import comet_ml \n",
    "from comet_ml import Experiment\n",
    "\n",
    "# Packages for text manipulation and Natural language processing\n",
    "import re\n",
    "from string import punctuation\n",
    "import nltk\n",
    "nltk.download(['stopwords','punkt'])\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# Train-test split package\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "# Libraries for data preparation and model building and evaluations\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from imblearn.pipeline import pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an experiment with your api key\n",
    "experiment = Experiment(\n",
    "    api_key=\"KQ1UTh7hBvPLWlz3034oIgusG\",\n",
    "    project_name=\"global-warming-climate-change-sentiment-analysis-zm3\",\n",
    "    workspace=\"okonp07@-gmail-com\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem statement\n",
    "\n",
    "Before a company begins to develop products for a segment of the population that believes in climate change and the effects of climate change on the environment and world, such a company must first establish the existence of such a demography. Even when it is clear that a section of the population indeed believs that there is a threat of environmental change, we must assess such factors as how much of a threat they believe climate change is. How far they may be willing to go to protect the environment and how passionate they are about supporting the efforts of others to fight climate change.\n",
    "\n",
    "By this experiment, we aim to explore machine learning as a method to assist us in identifying whether or not a person believes in climate change and could possibly be converted to a new customer based on their tweets. We will develop ML models that are capable of classifying tweets leaning positively towards a believe that Climate change is actuallya problem  and hence desirous of proferring a solution to this problem or leaning negatively towards that believe, hence treating climatic change as no threat. To produce such these models, we must first select an appropriate set of documents for training that has been annotated. Then, we pre-process and clean these documents, to enable us generate a feature and target set from them. We will then select relevant ML models, on which to train these datasets and evaluate their performance. Model evaluation is done both by comparing model predictions against a human panel at block level and comparing model performance against data that have been annotated but not used for training using cross-fold validation. Once a satisfactory performance of the model has been achieved, we interpret the patterns learned and apply them for further decision-making in the context of the experiment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries\n",
    "In the following cell, we shall import the libraries that are neccesary for us to use in completing the project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data\n",
    "To load your data, first ensure that the raw data and the notebook file are in the same folder on your local machine. The code below will load both the train and test data set into your notebook. If the files are not in the same folder, you will have to point to the directory in your machine or cloud location where the file is located. After loading your data, it is good practice to call up the loaded data just to verify that the data actually loaded as it should."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the train and test data sets from their respective CSV files\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test_with_no_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"EDA\"></a>\n",
    "### Exploratory Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ALL EDA / DATA PRE-PROCESSING HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not combine the train and test data in processing. carry out analysis to show insights that may be beneficial in explaining the sentiments shown in user tweets. The data pre-processing should be split into Univariate and multi variate analysis. Your EDA must tell the sory of the data. Some useful questions: \n",
    "* What is the sample size?\n",
    "* What key- words are useful to establish sentiments?\n",
    "* What are the sources of the data (News with verifiable sources, Informal tweets, etc )\n",
    "* Establish sentiments and their percentages in the data (Pro, Anti, Neutral, etc)\n",
    "* Check for words most commonly featured in the dataset\n",
    "* Frwequent Hashtags (Pro and anti)\n",
    "* Consider other conditionalities that will generate insiteful Visuals for you data and use them. \n",
    "\n",
    "Remember that EDA is all about visual presentation. Use visuals to tell the Story of your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Wrangling Function\n",
    "The following function will clean and preprocess any tweet parsed into it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_preprocessing(tweet):\n",
    "    \n",
    "    '''\n",
    "    This functions cleans tweets from line breaks, URLs, numbers, etc.\n",
    "    '''\n",
    "    \n",
    "    tweet = tweet.lower() #to lower case\n",
    "    tweet = tweet.replace('\\n', ' ') # remove line breaks\n",
    "    tweet = tweet.replace('\\@(\\w*)', '') # remove mentions\n",
    "    tweet = re.sub(r\"\\bhttps://t.co/\\w+\", '', tweet) # remove URLs\n",
    "    tweet = re.sub('\\w*\\d\\w*', '', tweet) # remove numbers\n",
    "    tweet = re.sub(r'\\#', '', tweet) # remove hashtags. To remove full hashtag: '\\#(\\w*)'\n",
    "    tweet = re.sub('\\w*\\d\\w*', '', tweet) # removes numbers?\n",
    "    tweet = re.sub(' +', ' ', tweet) # remove 1+ spaces\n",
    "\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test Split\n",
    "After we create a function for preprocessing we must split the data into labels and features (X and y) in order to enable us the run the models on our data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the labels and features\n",
    "train['processed'] = train['message'].apply(tweet_preprocessing)\n",
    "X = train['processed'].values\n",
    "y = train['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess testing data by applying our function\n",
    "test['processed'] = test['message'].apply(tweet_preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"feature\"></a>\n",
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the labels and fetures into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10,random_state=42,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = Pipeline([('Count',CountVectorizer()),('classify',MultinomialNB())])\n",
    "#fitting the model\n",
    "mnb.fit(X_train, y_train)\n",
    "\n",
    "#apply model on test data\n",
    "y_pred_mnb = mnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print(classification_report(y_test, y_pred_mnb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the '-1' and '0' class are poorly predicted when using unbalanced data. Once we implement resampling their f1-score increases for these model but only slightly. While at the same time the overall accuracy is slightly reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"modelling\"></a>\n",
    "# MODELLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC and LinearSVC\n",
    "\n",
    "SVC Provides a best fit to catergorize our data this fit can be nonlinear, while a linearSVC provides a linear interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVC\n",
    "svc = Pipeline([('Count',CountVectorizer()),('classify',SVC(max_iter=300,C=1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linearSVC\n",
    "linsvc = Pipeline([('Count',CountVectorizer()),('classify',LinearSVC(max_iter=300,C=1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Models the discrete probability distribution between classes and classifies based on the inflection point of the curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "lr = Pipeline([('Count',CountVectorizer()),('classify',LogisticRegression(max_iter=300))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN\n",
    "The KNN classifier assumes that all data points that similar data points tend to form clusters, close together. It classifies points that are close into the same class.K is the number of neighbours. So K=3 implies we will make our predictions based off of the 3 closest points to the data point beign assessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the KNN classifier\n",
    "knn = Pipeline([('Count',CountVectorizer()),('classify',KNeighborsClassifier(n_neighbors=3))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "\n",
    "The decision tree uses a tree-like model of decisions and their possible consequences.Starting from the decision itself (called a \"node\"), each branch of the decision tree represents a possible decision, outcome, or reaction and it works up until there is only one possible outcome left.\n",
    "<div align=\"center\" style=\"width: 500px; font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://www.aihr.com/wp-content/uploads/decision-trees-in-analytics.png\"\n",
    "     alt=\"Dummy image 1\"\n",
    "     style=\"float: center; padding-bottom=0.5em\"\n",
    "     width=500px/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree\n",
    "dt = Pipeline([('Count',CountVectorizer()),('classify',DecisionTreeClassifier())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "Using the decision tree as a base estimator,each estimator is trained on a different bootstrap sample having the same size as the training set. At each node of the forest, features are sampled without replacement to increase randomization. Nodes are split to maximise information gain.   \n",
    "\n",
    "<div align=\"center\" style=\"width: 500px; font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2020/05/rfc_vs_dt11.png\"\n",
    "     alt=\"Dummy image 1\"\n",
    "     style=\"float: center; padding-bottom=0.5em\"\n",
    "     width=500px/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call up the Random Forest Sampler\n",
    "rf = Pipeline([('Count',CountVectorizer()),('classify',RandomForestClassifier())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num=3\n",
    "# SVC\n",
    "scores = cross_val_score(\n",
    "        svc, X, y, cv=num, scoring='f1_weighted')\n",
    "print('The average weighted F1 score over '+str(num)+' SVC models is ' + str(sum(scores)/len(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linearSVC\n",
    "scores = cross_val_score(\n",
    "        linsvc, X, y, cv=num, scoring='f1_weighted')\n",
    "print('The average weighted F1 score over '+str(num)+ ' LinearSVC models is ' + str(sum(scores)/len(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "scores = cross_val_score(\n",
    "        lr, X, y, cv=num, scoring='f1_weighted')\n",
    "print('The average weighted F1 score over '+str(num)+' Logistic Regression models is ' + str(sum(scores)/len(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN\n",
    "scores = cross_val_score(\n",
    "        knn, X, y, cv=num, scoring='f1_weighted')\n",
    "print('The average weighted F1 score over '+str(num)+' KNN models is ' + str(sum(scores)/len(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree\n",
    "scores = cross_val_score(\n",
    "        dt, X, y, cv=num, scoring='f1_weighted')\n",
    "print('The average weighted F1 score over '+str(num)+' Decision Tree models is ' + str(sum(scores)/len(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "scores = cross_val_score(\n",
    "        rf, X, y, cv=num, scoring='f1_weighted')\n",
    "print('The average weighted F1 score over '+str(num)+' KNN models is ' + str(sum(scores)/len(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Logistic Regression Model and the LinearSVC model perform the best. The best performance for every model is found when resampling is not done. This could be because because upsampling the minority classes to the level of the majority class results in too much overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning parameters\n",
    "\n",
    "We take a look and see if we can improve our best 2 models: linearSVC and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "Cs = [0.001, 0.01, 0.1, 1, 10]\n",
    "param_grid = {\n",
    "    'C'     : Cs\n",
    "    }\n",
    "grid_SVM = GridSearchCV(LogisticRegression(), param_grid, scoring='f1_weighted', cv=3)\n",
    "grid_SVM.fit(CountVectorizer().fit_transform(X), y)\n",
    "grid_SVM.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C'     : Cs }\n",
    "grid_SVM = GridSearchCV(LinearSVC(), param_grid, scoring='f1_weighted', cv=3)\n",
    "grid_SVM.fit(CountVectorizer().fit_transform(X), y)\n",
    "grid_SVM.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conclusion\"></a>\n",
    "# Conclusion\n",
    "\n",
    "#### Model performance\n",
    "Several strategies we attempted to improve model performance, ranging from data processing techniques to clean the tweets, data balancing strategies, cross validation and grid search for the best values for model hyperparameters.\n",
    "\n",
    "\n",
    "\n",
    "### What else we can try\n",
    "Language models and the use of neural networks were two other strategies that we may implement to see how it will compare with essemble tree based models. It will also provide an opportunity for us to see how neural network models perforn on Natural Language tasks.\n",
    "\n",
    "### Business case value\n",
    "\n",
    "The analysis shows generally that the sentiment from the negative tweets arise from a a class of people who believe that climate change and the incidence of global warming is a hoax, or at best overated. Most of these persons defend their ideology with strong resolve and it will therefore be counter productive to try to market environmentally friendly products to them as an effort towards sustainability or to combat the incidence of Climate change. This class are more likely to become customers if other aspects of the product is promoted to them. They are more likely to purchase a product because of quality, fairness in price, use case etc rather than just because it is sustainably produced or good for the environmenmt.\n",
    "\n",
    "Conversely, People from the other end of the spectrum who display positive sentiments towards climate change definitely believe that climate change is an issue. They show some willingness to \"do something\" to play their role in combating this issue. What is yet unclear is if their sentiments would translate to any meaningful influence on their product aquisition habbits. They are definitely a better group to target with promotions that highlights the sustainability and environmenmtal friendliness of the products. To be safe though, this message should be embedded in other qualities of the product so that the environmental friendliness would be the Icing on the cake. It would be great for them if they have a product which is good already but also is sustainably produced. \n",
    "\n",
    "\n",
    "It will be beneficial for companies or organizations to band together and form groups where Some organisations are mentioned in the tweets, many which share the same values and ideals when it comes to protecting the environment, who have a substantial membership and following on social media of individuals who share the same values and ideals. The formation of potential partnerships with these organisations could lead to brand exposure with individuals who in their daily lives make conscious decisions with regards to the products and services they purchase.\n",
    "\n",
    "We recommend that the latter strategy of pursuing partnerships with like minded organisations will yield the best results, in terms of finding a group of potential customers who share the same values and ideals, and would be likely to purchase your products and services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"save\"></a>\n",
    "# SUBMISSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our final model, we build a stacking classifier to combine Logistic Regression, LinearSVC and Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [\n",
    "       ('rf', Pipeline([('Count',CountVectorizer(ngram_range=(1,2))),('classify',RandomForestClassifier())])),\n",
    "         \n",
    "        ('lnsvc', Pipeline([('Count',CountVectorizer(ngram_range=(1,2))),('classify',LinearSVC(C=0.1))])),\n",
    "         \n",
    "        ('MNB',Pipeline([('Count',CountVectorizer()),('classify',MultinomialNB())])),\n",
    "    \n",
    "        ('lr', Pipeline([('Count',CountVectorizer(ngram_range=(1,2))),('classify',LogisticRegression(C=1))]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = StackingClassifier(\n",
    "        estimators=estimators\n",
    "    )\n",
    "\n",
    "#fitting the model\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End experiment\n",
    "experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results on comet page\n",
    "experiment.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the unseen set, so that we can post to Kaggle and recieve a score based on the performance\n",
    "x_unseen = test['processed']\n",
    "\n",
    "submission = pd.DataFrame(\n",
    "    {'tweetid': test['tweetid'],\n",
    "     'sentiment': clf.predict(x_unseen)\n",
    "    })\n",
    "\n",
    "# save DataFrame to csv file for submission\n",
    "submission.to_csv(\"Submission_final.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
