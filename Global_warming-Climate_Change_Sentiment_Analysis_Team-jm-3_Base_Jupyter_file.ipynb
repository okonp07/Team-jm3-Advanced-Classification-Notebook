{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\"color: #000099\"> <h1><b>Climate Change Belief Analysis 2022</b></h1>\n",
    "    <h4> By </h4>\n",
    "    <h2>Datafluent Inc.</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Datafluent Inc.\n",
    "In this era of `Information Technology (IT)`, the variety and quantity of data generated on a daily basis is unprecedented. Data, which is sometimes referred to as the new oil, holds invaluable insights that can make the world a better place and businesses with growth mindsets have started exploiting this invaluable resource to keep thriving in the business world.\n",
    "\n",
    "Behold **`Datafluent Inc.!`** We are an organisation that enjoys squeezing out every bit of actionable insights from a variety of raw facts and communicating such insights in digestible chunks to our stakeholders. We are a team of five seasoned professionals that excelled in various disciplines. \n",
    "\n",
    "## Meet the Team\n",
    "\n",
    "<div align=\"center\" style=\"width: 900px; font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://github.com/okonp07/Team-jm3-Advanced-Classification-Base-Notebook/blob/main/The%20team.JPG\"\n",
    "     alt = \"The team\"\n",
    "     style=\"float: center; padding-bottom=0.5em\"\n",
    "     width=900px/>\n",
    "</div>\n",
    "\n",
    "## Project Overview\n",
    "Industrialization is the enabler of modern economic growth and development. However, this comes at the cost of emitting greenhouse gases that contribute, negatively, to climate change and ultimately global warming. Governments, eco-conscious organizations, and civil societies around the world are constantly exploring ways to reduce their carbon footprints. Despite many indices pointing towards climate change, many people are of the belief that climate change is a hoax. Datafluent Inc. is a corporate entity that specializes in mining out actionable insights from raw facts. In this project, we are contracted to use the novel tweets of some individuals to build various Machine Learning (ML) models to identify their beliefs about climate change. These models will be evaluated using the metrics: ***`accuracy, precision, recall, and F1-scores`*** to choose the optimal model. The optimal model’s outcome will help our client to predict how their eco-friendly products will be received by their prospective customers and thus enable them to make strategic business decisions.\n",
    "\n",
    "## Table of Contents\n",
    "<a href=#one>1.0 Introduction</a>\n",
    "- <a href=#1.1>1.1 Project Overview</a>\n",
    "- <a href=#1.2>1.2 Installing Dependencies and Importing Packages</a>\n",
    "- <a href=#1.3>1.3 Loading the Data</a>\n",
    "\n",
    "<a href=#two>2.0 Exploratory Data analysis</a>\n",
    "- <a href=#2.1>2.1 Data Wrangling</a>\n",
    "\n",
    "<a href=#three>3.0 Model Building and Evaluations</a>\n",
    "- <a href=#3.1>3.1 Train-test Split</a>\n",
    "- <a href=#3.2>3.2 Feature Selection</a>\n",
    "- <a href=#3.3>3.3 Model Building</a>\n",
    "- <a href=#3.4>3.4 Model Evaluation and Performance</a>\n",
    "- <a href=#3.5>3.5 Parameter Tunings</a>\n",
    "\n",
    "<a href=#four>4.0 Conclusion</a>\n",
    "\n",
    "<a href=#five>5.0 Submission</a>\n",
    "\n",
    "<a href=#six>References</a>\n",
    "\n",
    "<a id=\"one\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"one\"></a>\n",
    "# 1.0 Introduction\n",
    "Climate change is a big issue that needs serious attention and many eco-conscious organisations are exploring ways to build eco-friendly products. However, the populace is divided in their beliefs about climate change. **`Datafluent Inc.`** was hired by one such eco-conscious company to build a solution that determines the proportion of the population that believes in climate change. The information that the solution extracts will serve as a basis for the organisation strategic decision-making in building eco-friendly products. In this section, we are going to set the stage for our project on climate change belief analysis of about 16,000 tweets of some individuals. We will also explore the `Machine Learning (ML) tools` we will use in building the sentiment analysis model.\n",
    "\n",
    "## 1.1 Global Climate Change\n",
    "Global climate change refers to the average long-term change of climatic conditions over the entire earth. These include warming temperatures and changes in the precipitation as well as the effects of earth's warming such as:\n",
    "\n",
    "* rising sea levels\n",
    "* shrinking mountain glaciers\n",
    "* ice melting at a faster rate than usual in Greenland and Artic\n",
    "* wildfires\n",
    "* floods\n",
    "* droughts\n",
    "* famine\n",
    "* increasing desertification etc.\n",
    "\n",
    "There is a consensus in the scientific community that the earth has been getting warmer in the recent past and this has been linked to human activities, particularly the use of fossil fuels. The dangers of climate change spark debate among people, as we will see when exploring the tweets.  \n",
    "\n",
    "Earth’s climate has constantly been changing, even long before humans came into the picture. However, scientists have observed unusual changes recently. For example, Earth’s average temperature has been increasing much more quickly than they would expect over the past 150 years as shown in figure 1.1 below. According to scientists, the main human activities that contribute to climate change over the last century are the burning of fossil fuels like coal and oil as well as carbon pollution, which has increased the concentration of atmospheric carbon dioxide(CO2), which is a known greenhouse gas.\n",
    "<hr>\n",
    "<div align=\"center\" style=\"width: 500px; font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"World_Climate_Change_Chart.png\"\n",
    "     alt=\"Dummy image 1\"\n",
    "     style=\"float: center; padding-bottom=0.5em\"\n",
    "     width=900px/>|\n",
    "    <div align=\"center\">Figure 1.1: World Climate Change Chart</div>\n",
    " </div>\n",
    "\n",
    "## 1.2 Sentiment Analysis\n",
    "Sentiment analysis is a way of using natural language processing (NLP) and ML tools in identifying opinions expressed in texts. These opinions can be classified as positive (scored as 1), negative (scored as -1), or neutral (scored as 0). Sentiment analysis is important to businesses because it can reveal insights that can be used in improving sales, customer retention, timely problem identification, evaluating customers’ product perceptions, and exploring business opportunities.\n",
    "\n",
    "In this regard, people can be classified into three based on their perceptions about the causes of climate change. Thus,\n",
    "1. Pro-sentiment class: they believe that humans contribute to climate change\n",
    "2. Anti-sentiment class: these are those who do not believe in climate change and those that don’t believe that humans contribute to climate change\n",
    "3. Neutral class: these are those who take a neutral stand on the matter, displaying no strong positive or negative sentiment.\n",
    "\n",
    "### *1.2.1 Techniques of sentiment analysis*\n",
    "Sentiment analysis can be realized in two ways: rule-based and ML sentiment analysis. In rule-based analysis, manual NLP rules like lexicons (lists of words), stemming, tokenization, and parsing are used to simplify complex texts for computers to easily decipher opinions. Whereas, in ML sentiment analysis classification techniques like Naive Bayes, Logistic Regression, Support Vector Machines, Linear Regression, and Deep Learning are used to classify the contextual opinions.\n",
    "\n",
    "Quality insights are key to data-driven decision-making. At **`Datafluent Inc.`** we strongly believe in extracting only quality insights. Thus, we are going to analyze the given dataset thoroughly using both rule-based and ML techniques of sentiment analysis. This will ensure that no insight is left unturned in informing the decision-making process of our **`esteemed clients.`**\n",
    "\n",
    "## 1.3 Classification Techniques\n",
    "We are going to explore the dataset using the following models, compare them and choose the best amongst them.\n",
    "- Decision Tree Classifier\n",
    "- Random Forest Classifier\n",
    "- Linear Support Vector Classifier (LinearSVC)\n",
    "- Support Vector Classifier (SVC)\n",
    "- Logistic Regression\n",
    "- Stochastic Gradient Descent (SGD)\n",
    "\n",
    "### *1.3.1 Decision tree classifier*\n",
    "Decision Trees (DTs) are non-parametric supervised learning methods used for classification and regression. Decision trees learn from data to approximate a sine curve with a set of if-then-else decision rules as illustrated in figure 1.2 below. The deeper the tree, the more complex the decision rules, and the fitter the model.\n",
    "\n",
    "<div align=\"center\" style=\"width: 500px; font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"http://www.wordtemplatesdocs.org/wp-content/uploads/2017/11/Decision-Tree-template-free-00004.jpg\"\n",
    "     alt=\"Decision Tree\"\n",
    "     style=\"float: center; padding-bottom=0.5em\"\n",
    "     width=900px/>\n",
    "    <div align=\"center\">Figure 1.2: Decision Tree</div>\n",
    " </div>\n",
    "\n",
    "A decision tree builds classification or regression models in the form of a tree structure. It breaks down data by partitioning it into subsets after each decision while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes. A decision node has two or more branches. A leaf node represents a classification or decision. The topmost decision node in a tree that corresponds to the best predictor is called the root node. Decision trees can handle both categorical and numerical data.\n",
    "\n",
    "Decision trees are prone to overfitting. Overfitting happens when the learning algorithm continues to develop hypotheses that reduce training set error at the cost of an increased test set error; One method to tackle overfitting in decision trees is by pruning. \n",
    "\n",
    "There are several approaches used to avoid overfitting in building decision trees namely,\n",
    "- Pre-pruning stops growing the tree earlier, before it perfectly classifies the training set.\n",
    "- Post-pruning allows the tree to perfectly classify the training set, and then post-prune the tree. \n",
    "Practically, the second approach of post-pruning overfits trees is more successful because it is not easy to precisely estimate when to stop growing the tree.\n",
    "\n",
    "Decision Trees are building blocks for the next machine learning method we will look into, which is the Random Forest Classifier\n",
    "\n",
    "### *1.3.2 Random Forest Classifier*\n",
    "Random forest classifier is a Supervised Machine Learning Algorithm that is used widely in classification problems. It builds decision trees on different samples and takes their majority vote for classification. It is said that the more trees it has, the more robust a forest is. Unlike decision trees Random Forest prevents overfitting by creating trees on random subsets .\n",
    "\n",
    "The Random Forest algorithm works in four steps as follows:\n",
    "1. select a number of random samples from a given dataset\n",
    "2. construct a decision tree for each sample and get a prediction result from each decision tree\n",
    "3. perform a vote for each predicted result\n",
    "4. select the prediction result with the most votes as the final prediction.\n",
    "\n",
    "Random Forest Classifier is illustrated in figure 1.3 below:\n",
    "\n",
    "<div align=\"center\" style=\"width: 500px; font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://www.freecodecamp.org/news/content/images/2020/08/how-random-forest-classifier-work.PNG\"\n",
    "     alt=\"Decision Tree\"\n",
    "     style=\"float: center; padding-bottom=0.5em\"\n",
    "     width=900px/>\n",
    "    <div align=\"center\">Figure 1.3: Random Forest Classifier</div>\n",
    " </div>\n",
    "\n",
    "### *1.3.3 Support Vector Machine (SVM)*\n",
    "Support Vector Machine (SVM) is a linear model that can be used for classification problems. The objective of a Linear SVC (Support Vector Classifier) is to fit to the data you provide, returning a \"best fit\" hyperplane that divides, or categorizes, your data. After getting the hyperplane, you can then feed some features to your classifier to see what the \"predicted\" class is. It can solve linear and non-linear problems and works well for many practical problems. The idea of SVM is simple: The algorithm creates a line or a hyperplane which separates the data into classes as seen in the figure 1.4 below:\n",
    "\n",
    "\n",
    "<div align=\"center\" style=\"width: 500px; font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://pythonmachinelearning.pro/wp-content/uploads/2017/10/SVM-RBF-Iris.png\"\n",
    "     alt=\"Decision Tree\"\n",
    "     style=\"float: center; padding-bottom=0.5em\"\n",
    "     width=900px/>\n",
    "    <div align=\"center\">Figure 1.4: Support Vector Machine</div>\n",
    " </div>\n",
    " \n",
    "In the above example, the data is classified into categories that are represented by the blue, white and red class in line with pre-determined conditions stipulated by the SVM algorithm once that is completed, the data is sorted into classes as the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context of the Problem\n",
    "\n",
    "    \n",
    "Sentiment analysis refers to the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information.\n",
    " |\n",
    "![twitter-bird-animated-logo.gif?w=300&zoom=2](https://dropnerblog.files.wordpress.com/2019/12/twitter-bird-animated-logo.gif?w=300&zoom=2)\n",
    "    \n",
    "## Context of the Problem\n",
    "Companies are constructed around lessening one's environmental impact or carbon footprint. They offer products and services that are environmentally friendly and sustainable, in line with their values and ideals. They would like to determine how people perceive climate change and whether or not they believe it is a real threat. Before a company begins to develop products for a segment of the population that believes in climate change and the effects of climate change on the environment and world, it is logical for such a company to first establish the existence of such demography. Even when it is clear that a section of the population indeed believes that there is a threat of environmental change, we must assess such factors as how much of a threat they believe climate change is. How far they may be willing to go to protect the environment and how passionate they are about supporting the efforts of others to fight climate change.\n",
    "\n",
    "In this experiment, we aim to explore machine learning as a method to assist us in identifying whether or not a person believes in climate change and ascertain if such a person could be converted to a new customer based on their tweets. We will develop ML models that are capable of classifying tweets leaning positively towards a belief that Climate change is a problem and hence desirous of proferring a solution to this problem or leaning negatively towards that belief, hence treating climatic change as no threat. To produce these models, we must first select an appropriate set of data for training that has been annotated. Then, we pre-process and clean these data, to enable us to generate a feature and target set from them. We will then select relevant ML models, on which to train these datasets and evaluate their performance. Model evaluation is done by comparing model predictions against data that have been annotated but not used for training. Once a satisfactory performance of the model has been achieved, we interpret the patterns learned and apply them for further decision-making in the context of the experiment.\n",
    "\n",
    "<div align=\"center\" style=\"width: 950px; font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://www.itu.int/en/mediacentre/backgrounders/PublishingImages/climate-change-backgrounder.jpg\"\n",
    "     alt=\"Dummy image 1\"\n",
    "     style=\"float: center; padding-bottom=0.5em\"\n",
    "     width=950px/>\n",
    "    </div>\n",
    "\n",
    "    ### Problem Statement\n",
    "    Create a Natural Language Processing model to classify whether or not a person believes in climate change, based on their novel tweet data.\n",
    "    \n",
    "    ## Comet ML as a tool for Data Science\n",
    "   <a id=\"comet\"></a>\n",
    "# Comet ML as a tool for Data Science\n",
    "<img src=\"https://d15shllkswkct0.cloudfront.net/wp-content/blogs.dir/1/files/2021/11/cometml.png\" width=\"1000\" alt=\"Drawing\" style=\"width: 1000px;\"/>\n",
    "We will be using Comet to facilitate the model building process as well as a form of version control throughout the development of our model. To begin, we will first install the dependency that allows comet ML to integrate with our notebook and log all changes in our ML model as independent experiments in our comet account. This will allow us to have different versions of our model and will be really helpful in assessing the different properties and functionality of our model versions. This process is done in 3 distinct phases, Pip installation, importation of the experiment, and linking our notebook with our comet account. These processes are completed in the next three cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1.12\"></a>\n",
    "# 2.0 Exploratory Data Analysis\n",
    "In this section, we are going to build and evaluate several ML models and select the best amongst them.\n",
    "## 2.1 Installing Dependencies and Importing Packages\n",
    "In order to successfully build the models, there is a need to pip install some dependencies. Thus, we install the *autotime library, Comet, imblearn, and nltk.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: comet_ml in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (3.31.3)\n",
      "Requirement already satisfied: requests-toolbelt>=0.8.0 in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from comet_ml) (0.9.1)\n",
      "Requirement already satisfied: dulwich!=0.20.33,>=0.20.6 in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from comet_ml) (0.20.43)\n",
      "Requirement already satisfied: websocket-client>=0.55.0 in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from comet_ml) (1.3.2)\n",
      "Requirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from comet_ml) (4.4.0)\n",
      "Requirement already satisfied: requests>=2.18.4 in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from comet_ml) (2.27.1)\n",
      "Requirement already satisfied: wrapt>=1.11.2 in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from comet_ml) (1.12.1)\n",
      "Requirement already satisfied: semantic-version>=2.8.0 in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from comet_ml) (2.10.0)\n",
      "Requirement already satisfied: wurlitzer>=1.0.2 in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from comet_ml) (3.0.2)\n",
      "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from comet_ml) (7.352.0)\n",
      "Requirement already satisfied: everett[ini]>=1.0.1 in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from comet_ml) (3.0.0)\n",
      "Requirement already satisfied: six in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from comet_ml) (1.16.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from dulwich!=0.20.33,>=0.20.6->comet_ml) (2022.5.18.1)\n",
      "Requirement already satisfied: urllib3>=1.24.1 in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from dulwich!=0.20.33,>=0.20.6->comet_ml) (1.26.9)\n",
      "Requirement already satisfied: configobj in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from everett[ini]>=1.0.1->comet_ml) (5.0.6)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (21.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from requests>=2.18.4->comet_ml) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from requests>=2.18.4->comet_ml) (2.0.4)\n"
     ]
    }
   ],
   "source": [
    "# install comet\n",
    "!pip install comet_ml\n",
    "\n",
    "# importing experiment from Comet\n",
    "from comet_ml import Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/okonp07/global-warming-climate-change-sentiment-analysis-team-jm3/d5994fe4c25e4654b15e4f106f028d06\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Link your current workspace and create an experiment in comet with our api key:\n",
    "experiment = Experiment(\n",
    "    api_key=\"9smvwfgsHqWuSsKKueLCtsiBz\",\n",
    "    project_name=\"global-warming-climate-change-sentiment-analysis-team-jm3\",\n",
    "    workspace=\"okonp07\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting parfit\n",
      "  Downloading parfit-0.220.tar.gz (5.4 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from parfit) (1.1.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from parfit) (3.4.3)\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from parfit) (1.22.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from matplotlib->parfit) (1.4.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from matplotlib->parfit) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from matplotlib->parfit) (3.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from matplotlib->parfit) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from matplotlib->parfit) (9.0.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->parfit) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from sklearn->parfit) (1.1.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn->parfit) (1.7.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn->parfit) (2.2.0)\n",
      "Building wheels for collected packages: parfit, sklearn\n",
      "  Building wheel for parfit (setup.py): started\n",
      "  Building wheel for parfit (setup.py): finished with status 'done'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000018B10DB16A0>: Failed to establish a new connection: [Errno 11002] getaddrinfo failed')': /packages/08/60/8090b25f914c91914e8bb5c87819bfe52cead06b6d1a557f20afbe62c1ec/parfit-0.220.tar.gz\n",
      "  WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000018B10DB1CA0>: Failed to establish a new connection: [Errno 11002] getaddrinfo failed')': /packages/08/60/8090b25f914c91914e8bb5c87819bfe52cead06b6d1a557f20afbe62c1ec/parfit-0.220.tar.gz\n",
      "  WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000018B10DB1E50>: Failed to establish a new connection: [Errno 11002] getaddrinfo failed')': /packages/08/60/8090b25f914c91914e8bb5c87819bfe52cead06b6d1a557f20afbe62c1ec/parfit-0.220.tar.gz\n",
      "  WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000018B10DC5040>: Failed to establish a new connection: [Errno 11002] getaddrinfo failed')': /packages/08/60/8090b25f914c91914e8bb5c87819bfe52cead06b6d1a557f20afbe62c1ec/parfit-0.220.tar.gz\n",
      "  WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000018B10DC51F0>: Failed to establish a new connection: [Errno 11002] getaddrinfo failed')': /packages/08/60/8090b25f914c91914e8bb5c87819bfe52cead06b6d1a557f20afbe62c1ec/parfit-0.220.tar.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Created wheel for parfit: filename=parfit-0.220-py3-none-any.whl size=8658 sha256=c127212c6b43229068dea5ea99a47d464b70f6a2a0aefde953a78220d1bea852\n",
      "  Stored in directory: c:\\users\\buhari shehu\\appdata\\local\\pip\\cache\\wheels\\21\\0b\\03\\e9b81c9b318b5efa768bdf01c3aae5438a166d31f444bd800a\n",
      "  Building wheel for sklearn (setup.py): started\n",
      "  Building wheel for sklearn (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1310 sha256=c47cdcc31f8729654696a7032055a26633f59cc161e524c344919178fb8fb17a\n",
      "  Stored in directory: c:\\users\\buhari shehu\\appdata\\local\\pip\\cache\\wheels\\e4\\7b\\98\\b6466d71b8d738a0c547008b9eb39bf8676d1ff6ca4b22af1c\n",
      "Successfully built parfit sklearn\n",
      "Installing collected packages: sklearn, parfit\n",
      "Successfully installed parfit-0.220 sklearn-0.0\n",
      "Collecting scikit-plot\n",
      "  Downloading scikit_plot-0.3.7-py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from scikit-plot) (1.1.1)\n",
      "Requirement already satisfied: scipy>=0.9 in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from scikit-plot) (1.7.1)\n",
      "Requirement already satisfied: matplotlib>=1.4.0 in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from scikit-plot) (3.4.3)\n",
      "Requirement already satisfied: joblib>=0.10 in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from scikit-plot) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.16 in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (1.22.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (3.0.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (9.0.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (0.11.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=1.4.0->scikit-plot) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from scikit-learn>=0.18->scikit-plot) (2.2.0)\n",
      "Installing collected packages: scikit-plot\n",
      "Successfully installed scikit-plot-0.3.7\n",
      "Collecting contractions\n",
      "  Downloading contractions-0.1.72-py2.py3-none-any.whl (8.3 kB)\n",
      "Collecting textsearch>=0.0.21\n",
      "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
      "Collecting pyahocorasick\n",
      "  Downloading pyahocorasick-1.4.4-cp39-cp39-win_amd64.whl (39 kB)\n",
      "Collecting anyascii\n",
      "  Downloading anyascii-0.3.1-py3-none-any.whl (287 kB)\n",
      "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
      "Successfully installed anyascii-0.3.1 contractions-0.1.72 pyahocorasick-1.4.4 textsearch-0.0.21\n",
      "Collecting textblob\n",
      "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from textblob) (3.7)\n",
      "Requirement already satisfied: click in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (8.0.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.64.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2022.3.15)\n",
      "Requirement already satisfied: joblib in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.4)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.17.1\n",
      "Requirement already satisfied: pillow in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (9.0.1)\n"
     ]
    }
   ],
   "source": [
    "# installations\n",
    "!pip install parfit\n",
    "!pip install scikit-plot\n",
    "!pip install contractions\n",
    "!pip install -U textblob\n",
    "!pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for Natural Language  Processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pickle\n",
    "\n",
    "# imports for feature extractioin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# imports for preprocessing\n",
    "import contractions\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.probability import FreqDist\n",
    "#from wordcloud import WordCloud, ImageColorGenerator\n",
    "from PIL import Image\n",
    "from textblob import TextBlob\n",
    "\n",
    "# imports for classification models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# imports for hyperparameter tunning\n",
    "import parfit.parfit as pf\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# imports for metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# imports for data visualisation\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "from scikitplot.metrics import plot_roc, plot_confusion_matrix\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloud\n",
      "  Using cached wordcloud-1.8.1.tar.gz (220 kB)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from wordcloud) (1.22.4)\n",
      "Requirement already satisfied: pillow in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from wordcloud) (9.0.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from wordcloud) (3.4.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (3.0.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\buhari shehu\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Building wheels for collected packages: wordcloud\n",
      "  Building wheel for wordcloud (setup.py): started\n",
      "  Building wheel for wordcloud (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for wordcloud\n",
      "Failed to build wordcloud\n",
      "Installing collected packages: wordcloud\n",
      "    Running setup.py install for wordcloud: started\n",
      "    Running setup.py install for wordcloud: finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'C:\\Users\\Buhari Shehu\\anaconda3\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\Buhari Shehu\\\\AppData\\\\Local\\\\Temp\\\\pip-install-6g8r9q03\\\\wordcloud_9189f986a54649bda98b703b2db9a210\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\Buhari Shehu\\\\AppData\\\\Local\\\\Temp\\\\pip-install-6g8r9q03\\\\wordcloud_9189f986a54649bda98b703b2db9a210\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d 'C:\\Users\\Buhari Shehu\\AppData\\Local\\Temp\\pip-wheel-_jpk5u4q'\n",
      "       cwd: C:\\Users\\Buhari Shehu\\AppData\\Local\\Temp\\pip-install-6g8r9q03\\wordcloud_9189f986a54649bda98b703b2db9a210\\\n",
      "  Complete output (20 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-3.9\n",
      "  creating build\\lib.win-amd64-3.9\\wordcloud\n",
      "  copying wordcloud\\color_from_image.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "  copying wordcloud\\tokenization.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "  copying wordcloud\\wordcloud.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "  copying wordcloud\\wordcloud_cli.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "  copying wordcloud\\_version.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "  copying wordcloud\\__init__.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "  copying wordcloud\\__main__.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "  copying wordcloud\\stopwords -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "  copying wordcloud\\DroidSansMono.ttf -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "  UPDATING build\\lib.win-amd64-3.9\\wordcloud/_version.py\n",
      "  set build\\lib.win-amd64-3.9\\wordcloud/_version.py to '1.8.1'\n",
      "  running build_ext\n",
      "  building 'wordcloud.query_integral_image' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for wordcloud\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\Buhari Shehu\\anaconda3\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\Buhari Shehu\\\\AppData\\\\Local\\\\Temp\\\\pip-install-6g8r9q03\\\\wordcloud_9189f986a54649bda98b703b2db9a210\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\Buhari Shehu\\\\AppData\\\\Local\\\\Temp\\\\pip-install-6g8r9q03\\\\wordcloud_9189f986a54649bda98b703b2db9a210\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\Buhari Shehu\\AppData\\Local\\Temp\\pip-record-7rwty9rs\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\Users\\Buhari Shehu\\anaconda3\\Include\\wordcloud'\n",
      "         cwd: C:\\Users\\Buhari Shehu\\AppData\\Local\\Temp\\pip-install-6g8r9q03\\wordcloud_9189f986a54649bda98b703b2db9a210\\\n",
      "    Complete output (22 lines):\n",
      "    running install\n",
      "    C:\\Users\\Buhari Shehu\\anaconda3\\lib\\site-packages\\setuptools\\command\\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "      warnings.warn(\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build\\lib.win-amd64-3.9\n",
      "    creating build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\color_from_image.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\tokenization.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\wordcloud.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\wordcloud_cli.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\_version.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\__init__.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\__main__.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\stopwords -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\DroidSansMono.ttf -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "    UPDATING build\\lib.win-amd64-3.9\\wordcloud/_version.py\n",
      "    set build\\lib.win-amd64-3.9\\wordcloud/_version.py to '1.8.1'\n",
      "    running build_ext\n",
      "    building 'wordcloud.query_integral_image' extension\n",
      "    error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "    ----------------------------------------\n",
      "ERROR: Command errored out with exit status 1: 'C:\\Users\\Buhari Shehu\\anaconda3\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\Buhari Shehu\\\\AppData\\\\Local\\\\Temp\\\\pip-install-6g8r9q03\\\\wordcloud_9189f986a54649bda98b703b2db9a210\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\Buhari Shehu\\\\AppData\\\\Local\\\\Temp\\\\pip-install-6g8r9q03\\\\wordcloud_9189f986a54649bda98b703b2db9a210\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\Buhari Shehu\\AppData\\Local\\Temp\\pip-record-7rwty9rs\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\Users\\Buhari Shehu\\anaconda3\\Include\\wordcloud' Check the logs for full command output.\n"
     ]
    }
   ],
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Exploratory Data Analysis\n",
    "### 2.2.1 Load and view the data\n",
    "In this section, we are going to load the test and train datasets and view a snapshot of them to verify they are loaded correctly. We will also copy the datasets so that we can revert back to them in case of problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the train & test datasets\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test_with_no_labels.csv')\n",
    "\n",
    "# create copies for modelling\n",
    "train_df = train.copy()\n",
    "test_df = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
       "      <td>625221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
       "      <td>126103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @RawStory: Researchers say we have three ye...</td>\n",
       "      <td>698562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n",
       "      <td>573736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n",
       "      <td>466954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  tweetid\n",
       "0          1  PolySciMajor EPA chief doesn't think carbon di...   625221\n",
       "1          1  It's not like we lack evidence of anthropogeni...   126103\n",
       "2          2  RT @RawStory: Researchers say we have three ye...   698562\n",
       "3          1  #TodayinMaker# WIRED : 2016 was a pivotal year...   573736\n",
       "4          1  RT @SoyNovioDeTodas: It's 2016, and a racist, ...   466954"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check to be sure that the train data was properly imported as a pandas dataframe\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Europe will now be looking to China to make su...</td>\n",
       "      <td>169760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Combine this with the polling of staffers re c...</td>\n",
       "      <td>35326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The scary, unimpeachable evidence that climate...</td>\n",
       "      <td>224985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Karoli @morgfair @OsborneInk @dailykos \\nPuti...</td>\n",
       "      <td>476263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @FakeWillMoore: 'Female orgasms cause globa...</td>\n",
       "      <td>872928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  tweetid\n",
       "0  Europe will now be looking to China to make su...   169760\n",
       "1  Combine this with the polling of staffers re c...    35326\n",
       "2  The scary, unimpeachable evidence that climate...   224985\n",
       "3  @Karoli @morgfair @OsborneInk @dailykos \\nPuti...   476263\n",
       "4  RT @FakeWillMoore: 'Female orgasms cause globa...   872928"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check to be sure that the test data was properly imported as a pandas dataframe\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking the shape of the dataset**\n",
    "\n",
    "The shape of the data tells you the number of rows and columns that exists in a particular dataframe. In the next cell, we will examine the number of rows and columns present in the train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15819, 3), (10546, 2))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the number of rows and columns in the datasets\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next two cells, we will check the proportion of the entire dataset that is commited to the train and test set. We can establish this by checking the percentage of the entire data that is reserved as the train data and the proportion that is reserved as the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the percentage of the data committed as the train set\n",
    "((15819)/(15819+10546)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the percentage of the data committed as the test set\n",
    "((10546)/(15819+10546)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train data sets shape shows that there are 15819 rows and 3 columns in the train data. The columns are identified as `sentiment`, `message`, and `tweetid`. The test data has 10,546 rows and 2 columns. The columns in the test data set are identified as `message` and `tweetid`. The target (that is the variable that our model seeks to predict) can therefore be identified as the `Sentiment` value. This is why it is only present in the train set. In the course of the development of our model, we will use the \"message\" and the \"tweetid\" columns to engineer the features that will be used to predict the `Sentiment` of every tweet in the test data set.\n",
    "\n",
    "### 2.2.2 Data Preprocessing\n",
    "Usually, when data is collected in the real world, it comes with a lot of \"impurities\". Some data would come in formats that are different from the format that the data scientist desires, there may be null values, unrecognizable characters present in the data, etc. Data pre-processing refers to the process of removing these impurities from the data and taking the necessary steps to ensure that the data is in a state that will give clear and interpretable insights and also at its best to be used for the process of ML models. The process of Data pre-processing involves searching for these possible errors in your data and fixing them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15819 entries, 0 to 15818\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   sentiment  15819 non-null  int64 \n",
      " 1   message    15819 non-null  object\n",
      " 2   tweetid    15819 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 370.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# cheching if there are missing values in the train dataset\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10546 entries, 0 to 10545\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   message  10546 non-null  object\n",
      " 1   tweetid  10546 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 164.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# checking if there are missing values in the Test dataset\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that both the train and the test datasets are devoid of nulls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Prepare a function for Parts of Speech for modelling\n",
    "\n",
    "A Part-of-speech (POS) in ML modeling is a grammatical classification that commonly includes verbs (\"v\"), adjectives (\"a\"), adverbs(\"r\"), nouns(\"n\"), etc. POS tagging is an important natural language processing application used in machine translation, word sense disambiguation, question answering parsing, and so on. In the cell below, we write a function that extracts the parts of speech from a parsed entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def POS(word):\n",
    "    \"\"\"\n",
    "    This function gets the part of speech\n",
    "    \"\"\"\n",
    "    pos_counts = Counter()\n",
    "    probable_part_of_speech = wordnet.synsets(word)\n",
    "    pos_counts[\"n\"] = len([i for i in probable_part_of_speech if i.pos()==\"n\"])\n",
    "    pos_counts[\"v\"] = len([i for i in probable_part_of_speech if i.pos()==\"v\"])\n",
    "    pos_counts[\"a\"] = len([i for i in probable_part_of_speech if i.pos()==\"a\"])\n",
    "    pos_counts[\"r\"] = len([i for i in probable_part_of_speech if i.pos()==\"r\"])\n",
    "    part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "    return part_of_speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 Extract useful data\n",
    "\n",
    "The function below will generate new features from the dataset that will be useful in revealing more insights from the data. The function will also extract hashtags from the \"message\" column and place them in a new column called \"hashtags\". It will also extract mentions (where the tweet contains @mentions) and place them in a column called \"mentions\" and lastly, it will extract URLs where they are referenced and place them in a column called \"url\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractor(df):\n",
    "    \"\"\"\" \n",
    "        Returns a DataFrame with three additional columns: \n",
    "        \"hashtags\", \"mentions\" and \"url\"\n",
    "         Args:\n",
    "             DataFrame: DateFrame with Data,Index and Colums\n",
    "        Return:\n",
    "             DataFrame: A DataFrame with additional columns with data\n",
    "        Egs:\n",
    "             df['new colum'] = col_name\n",
    "    \"\"\"\n",
    "    # extract hashtags\n",
    "    tweets = df['message'] \n",
    "    df['hashtags'] = df.message.str.lower().str.findall(r'#.*?(?=\\s|$)') \n",
    "    htags = df['hashtags']\n",
    "    df['hashtags'] = htags.apply(lambda x: np.nan if len(x) == 0 else x)\n",
    "    \n",
    "    # extract mentions\n",
    "    df['mentions'] = df.message.str.lower().str.findall(r'@\\w*')\n",
    "    mtags = df['mentions']\n",
    "    df['mentions'] = mtags.apply(lambda x: np.nan if len(x) == 0 else x)\n",
    "    \n",
    "    # extract url\n",
    "    df['url'] = df.message. str.lower().str.findall(r'http\\S+|www.\\S+')\n",
    "    urltags = df['url']\n",
    "    df['url'] = urltags.apply(lambda x: np.nan if len(x) == 0 else x)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>mentions</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
       "      <td>625221</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[@mashable]</td>\n",
       "      <td>[https://t.co/yelvcefxkc]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
       "      <td>126103</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @RawStory: Researchers say we have three ye...</td>\n",
       "      <td>698562</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[@rawstory]</td>\n",
       "      <td>[https://t.co/wdt0kdur2f, https://t.co/z0anpt…]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n",
       "      <td>573736</td>\n",
       "      <td>[#todayinmaker#]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[https://t.co/44wotxtlcd]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n",
       "      <td>466954</td>\n",
       "      <td>[#electionnight]</td>\n",
       "      <td>[@soynoviodetodas]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  tweetid  \\\n",
       "0          1  PolySciMajor EPA chief doesn't think carbon di...   625221   \n",
       "1          1  It's not like we lack evidence of anthropogeni...   126103   \n",
       "2          2  RT @RawStory: Researchers say we have three ye...   698562   \n",
       "3          1  #TodayinMaker# WIRED : 2016 was a pivotal year...   573736   \n",
       "4          1  RT @SoyNovioDeTodas: It's 2016, and a racist, ...   466954   \n",
       "\n",
       "           hashtags            mentions  \\\n",
       "0               NaN         [@mashable]   \n",
       "1               NaN                 NaN   \n",
       "2               NaN         [@rawstory]   \n",
       "3  [#todayinmaker#]                 NaN   \n",
       "4  [#electionnight]  [@soynoviodetodas]   \n",
       "\n",
       "                                               url  \n",
       "0                        [https://t.co/yelvcefxkc]  \n",
       "1                                              NaN  \n",
       "2  [https://t.co/wdt0kdur2f, https://t.co/z0anpt…]  \n",
       "3                        [https://t.co/44wotxtlcd]  \n",
       "4                                              NaN  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing the function\n",
    "extractor(train_df).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>mentions</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
       "      <td>625221</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[@mashable]</td>\n",
       "      <td>[https://t.co/yelvcefxkc]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
       "      <td>126103</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @RawStory: Researchers say we have three ye...</td>\n",
       "      <td>698562</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[@rawstory]</td>\n",
       "      <td>[https://t.co/wdt0kdur2f, https://t.co/z0anpt…]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n",
       "      <td>573736</td>\n",
       "      <td>[#todayinmaker#]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[https://t.co/44wotxtlcd]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n",
       "      <td>466954</td>\n",
       "      <td>[#electionnight]</td>\n",
       "      <td>[@soynoviodetodas]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  tweetid  \\\n",
       "0          1  PolySciMajor EPA chief doesn't think carbon di...   625221   \n",
       "1          1  It's not like we lack evidence of anthropogeni...   126103   \n",
       "2          2  RT @RawStory: Researchers say we have three ye...   698562   \n",
       "3          1  #TodayinMaker# WIRED : 2016 was a pivotal year...   573736   \n",
       "4          1  RT @SoyNovioDeTodas: It's 2016, and a racist, ...   466954   \n",
       "\n",
       "           hashtags            mentions  \\\n",
       "0               NaN         [@mashable]   \n",
       "1               NaN                 NaN   \n",
       "2               NaN         [@rawstory]   \n",
       "3  [#todayinmaker#]                 NaN   \n",
       "4  [#electionnight]  [@soynoviodetodas]   \n",
       "\n",
       "                                               url  \n",
       "0                        [https://t.co/yelvcefxkc]  \n",
       "1                                              NaN  \n",
       "2  [https://t.co/wdt0kdur2f, https://t.co/z0anpt…]  \n",
       "3                        [https://t.co/44wotxtlcd]  \n",
       "4                                              NaN  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the transformation\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.5 Removing unnecessary information from the message\n",
    "Regular expression contains a series of characters that define a pattern of text to be matched to create filters for raw data that may be specialized, or general. Below, we shall use regex codes to clean the data. Recall that we have extracted the URL, mentions, and hashtag information from the original message column to form new columns of their own. We can now remove the information from the message column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Buhari Shehu\\AppData\\Local\\Temp\\ipykernel_7084\\210805835.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  train_df['message'] = train['message'].str.replace('http\\S+|www.\\S+', '', case=False)\n",
      "C:\\Users\\Buhari Shehu\\AppData\\Local\\Temp\\ipykernel_7084\\210805835.py:5: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  train_df['message'] = train['message'].str.replace('@\\w*', '', case=False)\n",
      "C:\\Users\\Buhari Shehu\\AppData\\Local\\Temp\\ipykernel_7084\\210805835.py:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  train_df['message'] = train['message'].str.replace('#.*?(?=\\s|$)', '', case=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>mentions</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
       "      <td>625221</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[@mashable]</td>\n",
       "      <td>[https://t.co/yelvcefxkc]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
       "      <td>126103</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Researchers say we have three years to act on...</td>\n",
       "      <td>698562</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[@rawstory]</td>\n",
       "      <td>[https://t.co/wdt0kdur2f, https://t.co/z0anpt…]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>WIRED : 2016 was a pivotal year in the war on...</td>\n",
       "      <td>573736</td>\n",
       "      <td>[#todayinmaker#]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[https://t.co/44wotxtlcd]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>It's 2016, and a racist, sexist, climate chan...</td>\n",
       "      <td>466954</td>\n",
       "      <td>[#electionnight]</td>\n",
       "      <td>[@soynoviodetodas]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  tweetid  \\\n",
       "0          1  PolySciMajor EPA chief doesn't think carbon di...   625221   \n",
       "1          1  It's not like we lack evidence of anthropogeni...   126103   \n",
       "2          2   Researchers say we have three years to act on...   698562   \n",
       "3          1   WIRED : 2016 was a pivotal year in the war on...   573736   \n",
       "4          1   It's 2016, and a racist, sexist, climate chan...   466954   \n",
       "\n",
       "           hashtags            mentions  \\\n",
       "0               NaN         [@mashable]   \n",
       "1               NaN                 NaN   \n",
       "2               NaN         [@rawstory]   \n",
       "3  [#todayinmaker#]                 NaN   \n",
       "4  [#electionnight]  [@soynoviodetodas]   \n",
       "\n",
       "                                               url  \n",
       "0                        [https://t.co/yelvcefxkc]  \n",
       "1                                              NaN  \n",
       "2  [https://t.co/wdt0kdur2f, https://t.co/z0anpt…]  \n",
       "3                        [https://t.co/44wotxtlcd]  \n",
       "4                                              NaN  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove URLs\n",
    "train_df['message'] = train['message'].str.replace('http\\S+|www.\\S+', '', case=False)\n",
    "\n",
    "# remove mentions\n",
    "train_df['message'] = train['message'].str.replace('@\\w*', '', case=False)\n",
    "\n",
    "# remove hashtags\n",
    "train_df['message'] = train['message'].str.replace('#.*?(?=\\s|$)', '', case=False)\n",
    "\n",
    "# remove 'RT'\n",
    "train_df['message'] = train['message'].str.replace('RT :', '', case=False)\n",
    "\n",
    "# viewing the data\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.6 Expand contractions\n",
    "Contractions are words or combinations of words that are shortened by dropping letters and replacing them with an apostrophe. Expanding contractions contribute to text standardization and are useful when we are working on Twitter data as the words play an important role in sentiment analysis. The following code string expands the message from the form containing contractions to the expanded form without the contractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['message'] = train_df['message'].apply(lambda x: contractions.fix(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.7 Tokenization\n",
    "Tokenization is the process of dividing text into a set of meaningful pieces called tokens. To further process the data, we shall build a function that will tokenize the data. The function will also clean the data by removing digits and single character tokens which may contribute nothing to text our interpretation efforts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(df):\n",
    "    '''\n",
    "    This function cleans the tweets by tokenizing, removing punctuation, \n",
    "    removing digits and removing 1 character tokens\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # tokenizing the tweets\n",
    "    clean_tweets = df['message'].apply(TweetTokenizer().tokenize) ## first we tokenize\n",
    "\n",
    "    # remove punctuation\n",
    "    clean_tweets = clean_tweets.apply(lambda x : [token for token in x if token not in string.punctuation])\n",
    "\n",
    "    # removing digits from the tweets\n",
    "    clean_tweets = clean_tweets.apply(lambda x: [token for token in x if token not in list(string.digits)])\n",
    "\n",
    "    # lastly we remove all one character tokens\n",
    "    clean_tweets = clean_tweets.apply(lambda x: [token for token in x if len(token) > 1])\n",
    "    \n",
    "    df['cleaned_tweets'] = clean_tweets\n",
    "    \n",
    "    return df['cleaned_tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [PolySciMajor, EPA, chief, does, not, think, c...\n",
       "1        [It, is, not, like, we, lack, evidence, of, an...\n",
       "2        [Researchers, say, we, have, three, years, to,...\n",
       "3        [WIRED, 2016, was, pivotal, year, in, the, war...\n",
       "4        [It, is, 2016, and, racist, sexist, climate, c...\n",
       "                               ...                        \n",
       "15814    [They, took, down, the, material, on, global, ...\n",
       "15815    [How, climate, change, could, be, breaking, up...\n",
       "15816    [notiven, RT, nytimesworld, What, does, Trump,...\n",
       "15817    [Hey, liberals, the, climate, change, crap, is...\n",
       "15818         [climate, change, equation, in, screenshots]\n",
       "Name: cleaned_tweets, Length: 15819, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input train data in 'clean_tweets' function to clean the tweets\n",
    "clean_tweets(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.8 Removing Stopwords\n",
    "Stop words are used in Text Mining and Natural Language Processing (NLP) to eliminate words that are so commonly used that they carry very little useful information for machine algorithms. For example, in the context of a search system, if your search query is “what is a stop word?”, you want the search system to focus on surfacing documents that talk about \"stop word\" over documents that talk about \"what is a\".The following code block defines the library to be used for stop word removal. It also removes the stop words from the train dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the stopword library to be used\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# convert to lower case\n",
    "train_df['cleaned_tweets'] = train_df['cleaned_tweets'].apply(lambda x: [word.lower() for word in x])\n",
    "\n",
    "# remove stopwords\n",
    "train_df['no_stopwords'] = train_df['cleaned_tweets'].apply(lambda x: [item for item in x if item not in stop])\n",
    "\n",
    "# Print out the resulting dataframe\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 33.8 s (started: 2022-06-21 11:39:12 +01:00)\n"
     ]
    }
   ],
   "source": [
    "# Installations\n",
    "%%capture\n",
    "!pip install ipython-autotime\n",
    "!pip install comet-ml\n",
    "!pip install imblearn --user\n",
    "!pip install --user -U nltk\n",
    "!pip install wordcloud\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install -c anaconda -c conda-forge -c comet_ml comet_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2022-06-21 11:56:10 +01:00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Buhari\n",
      "[nltk_data]     Shehu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Buhari\n",
      "[nltk_data]     Shehu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Libraries for data loading, data manipulation and data visulisation\n",
    "import numpy as np # for linear algebra\n",
    "import pandas as pd # for importing, creating and manipulating dataframes\n",
    "\n",
    "# Visualization Packages\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Warnings\n",
    "import warnings \n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Import comet_ml \n",
    "from comet_ml import Experiment\n",
    "\n",
    "# Packages for text manipulation and Natural language processing\n",
    "import re\n",
    "from string import punctuation\n",
    "import nltk\n",
    "nltk.download(['stopwords','punkt'])\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# Train-test split package\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "# Libraries for data preparation and model building and evaluations\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from imblearn.pipeline import pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an experiment with your api key\n",
    "experiment = Experiment(\n",
    "    api_key=\"KQ1UTh7hBvPLWlz3034oIgusG\",\n",
    "    project_name=\"global-warming-climate-change-sentiment-analysis-zm3\",\n",
    "    workspace=\"okonp07@-gmail-com\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1.3\"></a>\n",
    "## 1.3 Loading the Data\n",
    "To load your data, first ensure that the raw data and the notebook file are in the same folder on your local machine. The code below will load both the train and test data sets into your notebook. If the files are not in the same folder, you will have to point to the directory in your machine or cloud location where the file is located. After loading your data, it is good practice to call up the loaded data just to verify that the data actually loaded as it should."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 141 ms (started: 2022-06-21 15:09:07 +01:00)\n"
     ]
    }
   ],
   "source": [
    "#Load the train and test data sets from their respective CSV files\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test_with_no_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"two\"></a>\n",
    "# 2.0 Exploratory Data analysis\n",
    "\n",
    "In this section, we are going to objectively analyse the characterisitcs of our train and test data sets, independently, to have a grasp of the features of the data sets.\n",
    "\n",
    " Some useful questions: \n",
    "* What is the sample size?\n",
    "* What key- words are useful to establish sentiments?\n",
    "* What are the sources of the data (News with verifiable sources, Informal tweets, etc )\n",
    "* Establish sentiments and their percentages in the data (Pro, Anti, Neutral, etc)\n",
    "* Check for words most commonly featured in the dataset\n",
    "* Frwequent Hashtags (Pro and anti)\n",
    "* Consider other conditionalities that will generate insiteful Visuals for you data and use them. \n",
    "\n",
    "Remember that EDA is all about visual presentation. Use visuals to tell the Story of your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a glimpse into the datasets and their shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
       "      <td>625221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
       "      <td>126103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @RawStory: Researchers say we have three ye...</td>\n",
       "      <td>698562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n",
       "      <td>573736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n",
       "      <td>466954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  tweetid\n",
       "0          1  PolySciMajor EPA chief doesn't think carbon di...   625221\n",
       "1          1  It's not like we lack evidence of anthropogeni...   126103\n",
       "2          2  RT @RawStory: Researchers say we have three ye...   698562\n",
       "3          1  #TodayinMaker# WIRED : 2016 was a pivotal year...   573736\n",
       "4          1  RT @SoyNovioDeTodas: It's 2016, and a racist, ...   466954"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 407 ms (started: 2022-06-21 14:08:46 +01:00)\n"
     ]
    }
   ],
   "source": [
    "# first five rows of the dataset\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15819, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2022-06-21 14:09:38 +01:00)\n"
     ]
    }
   ],
   "source": [
    "# checking the shape of the dataset\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15819 entries, 0 to 15818\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   sentiment  15819 non-null  int64 \n",
      " 1   message    15819 non-null  object\n",
      " 2   tweetid    15819 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 370.9+ KB\n",
      "time: 782 ms (started: 2022-06-21 14:14:31 +01:00)\n"
     ]
    }
   ],
   "source": [
    "# checking for missing values and data types\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the above that the train dataset consists of 15819 tweets along with their sentiment scores. The dataset is deviod of any missing values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.1\"></a>\n",
    "### 2.1 Data Wrangling\n",
    "It is essential to clean and preprocess the datasets so as to build more accurate models.The following function will clean and preprocess any tweet parsed into it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 15 ms (started: 2022-06-21 14:45:05 +01:00)\n"
     ]
    }
   ],
   "source": [
    "def tweet_preprocessing(tweet):\n",
    "    \n",
    "    '''\n",
    "    This functions cleans tweets from line breaks, URLs, numbers, etc.\n",
    "    '''\n",
    "    \n",
    "    tweet = tweet.lower() #to lower case\n",
    "    tweet = tweet.replace('\\n', ' ') # remove line breaks\n",
    "    tweet = tweet.replace('\\@(\\w*)', '') # remove mentions\n",
    "    tweet = re.sub(r\"\\bhttps://t.co/\\w+\", '', tweet) # remove URLs\n",
    "    tweet = re.sub('\\w*\\d\\w*', '', tweet) # remove numbers\n",
    "    tweet = re.sub(r'\\#', '', tweet) # remove hashtags. To remove full hashtag: '\\#(\\w*)'\n",
    "    tweet = re.sub('\\w*\\d\\w*', '', tweet) # removes numbers?\n",
    "    tweet = re.sub(' +', ' ', tweet) # remove 1+ spaces\n",
    "\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"three\"></a>\n",
    "# 3.0 Model Building and Evaluations\n",
    "In this section, we are going to build and evalaute various ML models and select the best one amongst them. Will attempt improving the performance of our models by hyper parameter tuning and we will evaluate the performance of these in our dataset:\n",
    "- Naive Bayes Classifier\n",
    "- SVC and LinearSVC\n",
    "- Logistic Regression\n",
    "- KNN\n",
    "- Decision Tree\n",
    "- Random Forests\n",
    "\n",
    "<a id=\"3.1\"></a>\n",
    "## 3.1 Train-test Split\n",
    "After we create a function for preprocessing we must split the data into labels and features (X and y) in order to enable us the run the models on our data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.05 s (started: 2022-06-21 14:49:24 +01:00)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the labels and features\n",
    "train['processed'] = train['message'].apply(tweet_preprocessing)\n",
    "X = train['processed'].values\n",
    "y = train['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 641 ms (started: 2022-06-21 14:49:25 +01:00)\n"
     ]
    }
   ],
   "source": [
    "# preprocess testing data by applying our function\n",
    "test['processed'] = test['message'].apply(tweet_preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.2\"></a>\n",
    "## 3.2 Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.3\"></a>\n",
    "## 3.3 Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Naive Bayes Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 218 ms (started: 2022-06-21 14:50:34 +01:00)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the labels and fetures into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10,random_state=42,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = Pipeline([('Count',CountVectorizer()),('classify',MultinomialNB())])\n",
    "#fitting the model\n",
    "mnb.fit(X_train, y_train)\n",
    "\n",
    "#apply model on test data\n",
    "y_pred_mnb = mnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print(classification_report(y_test, y_pred_mnb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the '-1' and '0' class are poorly predicted when using unbalanced data. Once we implement resampling their f1-score increases for these model but only slightly. While at the same time the overall accuracy is slightly reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. SVC and LinearSVC\n",
    "\n",
    "SVC Provides a best fit to catergorize our data this fit can be nonlinear, while a linearSVC provides a linear interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVC\n",
    "svc = Pipeline([('Count',CountVectorizer()),('classify',SVC(max_iter=300,C=1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linearSVC\n",
    "linsvc = Pipeline([('Count',CountVectorizer()),('classify',LinearSVC(max_iter=300,C=1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Logistic Regression\n",
    "\n",
    "Models the discrete probability distribution between classes and classifies based on the inflection point of the curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "lr = Pipeline([('Count',CountVectorizer()),('classify',LogisticRegression(max_iter=300))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. KNN\n",
    "The KNN classifier assumes that all data points that similar data points tend to form clusters, close together. It classifies points that are close into the same class.K is the number of neighbours. So K=3 implies we will make our predictions based off of the 3 closest points to the data point beign assessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the KNN classifier\n",
    "knn = Pipeline([('Count',CountVectorizer()),('classify',KNeighborsClassifier(n_neighbors=3))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e. Decision Tree\n",
    "\n",
    "The decision tree uses a tree-like model of decisions and their possible consequences.Starting from the decision itself (called a \"node\"), each branch of the decision tree represents a possible decision, outcome, or reaction and it works up until there is only one possible outcome left.\n",
    "<div align=\"center\" style=\"width: 500px; font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://www.aihr.com/wp-content/uploads/decision-trees-in-analytics.png\"\n",
    "     alt=\"Dummy image 1\"\n",
    "     style=\"float: center; padding-bottom=0.5em\"\n",
    "     width=500px/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree\n",
    "dt = Pipeline([('Count',CountVectorizer()),('classify',DecisionTreeClassifier())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f. Random Forest\n",
    "Using the decision tree as a base estimator,each estimator is trained on a different bootstrap sample having the same size as the training set. At each node of the forest, features are sampled without replacement to increase randomization. Nodes are split to maximise information gain.   \n",
    "\n",
    "<div align=\"center\" style=\"width: 500px; font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2020/05/rfc_vs_dt11.png\"\n",
    "     alt=\"Dummy image 1\"\n",
    "     style=\"float: center; padding-bottom=0.5em\"\n",
    "     width=500px/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call up the Random Forest Sampler\n",
    "rf = Pipeline([('Count',CountVectorizer()),('classify',RandomForestClassifier())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.4\"></a>\n",
    "### 3.4 Model Evaluation and Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'svc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m num\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# SVC\u001b[39;00m\n\u001b[0;32m      3\u001b[0m scores \u001b[38;5;241m=\u001b[39m cross_val_score(\n\u001b[1;32m----> 4\u001b[0m         \u001b[43msvc\u001b[49m, X, y, cv\u001b[38;5;241m=\u001b[39mnum, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1_weighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe average weighted F1 score over \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(num)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m SVC models is \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28msum\u001b[39m(scores)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(scores)))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'svc' is not defined"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 156 ms (started: 2022-06-21 15:27:09 +01:00)\n"
     ]
    }
   ],
   "source": [
    "num=3\n",
    "# SVC\n",
    "scores = cross_val_score(\n",
    "        svc, X, y, cv=num, scoring='f1_weighted')\n",
    "print('The average weighted F1 score over '+str(num)+' SVC models is ' + str(sum(scores)/len(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linearSVC\n",
    "scores = cross_val_score(\n",
    "        linsvc, X, y, cv=num, scoring='f1_weighted')\n",
    "print('The average weighted F1 score over '+str(num)+ ' LinearSVC models is ' + str(sum(scores)/len(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "scores = cross_val_score(\n",
    "        lr, X, y, cv=num, scoring='f1_weighted')\n",
    "print('The average weighted F1 score over '+str(num)+' Logistic Regression models is ' + str(sum(scores)/len(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN\n",
    "scores = cross_val_score(\n",
    "        knn, X, y, cv=num, scoring='f1_weighted')\n",
    "print('The average weighted F1 score over '+str(num)+' KNN models is ' + str(sum(scores)/len(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree\n",
    "scores = cross_val_score(\n",
    "        dt, X, y, cv=num, scoring='f1_weighted')\n",
    "print('The average weighted F1 score over '+str(num)+' Decision Tree models is ' + str(sum(scores)/len(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "scores = cross_val_score(\n",
    "        rf, X, y, cv=num, scoring='f1_weighted')\n",
    "print('The average weighted F1 score over '+str(num)+' KNN models is ' + str(sum(scores)/len(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Logistic Regression Model and the LinearSVC model perform the best. The best performance for every model is found when resampling is not done. This could be because because upsampling the minority classes to the level of the majority class results in too much overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.5\"></a>\n",
    "### 3.5 Parameter Tunings\n",
    "\n",
    "We take a look and see if we can improve our best 2 models: linearSVC and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "Cs = [0.001, 0.01, 0.1, 1, 10]\n",
    "param_grid = {\n",
    "    'C'     : Cs\n",
    "    }\n",
    "grid_SVM = GridSearchCV(LogisticRegression(), param_grid, scoring='f1_weighted', cv=3)\n",
    "grid_SVM.fit(CountVectorizer().fit_transform(X), y)\n",
    "grid_SVM.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C'     : Cs }\n",
    "grid_SVM = GridSearchCV(LinearSVC(), param_grid, scoring='f1_weighted', cv=3)\n",
    "grid_SVM.fit(CountVectorizer().fit_transform(X), y)\n",
    "grid_SVM.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"four\"></a>\n",
    "# 4.0 Conclusion\n",
    "\n",
    "### Model performance\n",
    "Several strategies we attempted to improve model performance, ranging from data processing techniques to clean the tweets, data balancing strategies, cross validation and grid search for the best values for model hyperparameters.\n",
    "\n",
    "\n",
    "\n",
    "### What else we can try\n",
    "Language models and the use of neural networks were two other strategies that we may implement to see how it will compare with essemble tree based models. It will also provide an opportunity for us to see how neural network models perforn on Natural Language tasks.\n",
    "\n",
    "### Business case value\n",
    "\n",
    "The analysis shows generally that the sentiment from the negative tweets arise from a a class of people who believe that climate change and the incidence of global warming is a hoax, or at best overated. Most of these persons defend their ideology with strong resolve and it will therefore be counter productive to try to market environmentally friendly products to them as an effort towards sustainability or to combat the incidence of Climate change. This class are more likely to become customers if other aspects of the product is promoted to them. They are more likely to purchase a product because of quality, fairness in price, use case etc rather than just because it is sustainably produced or good for the environmenmt.\n",
    "\n",
    "Conversely, People from the other end of the spectrum who display positive sentiments towards climate change definitely believe that climate change is an issue. They show some willingness to \"do something\" to play their role in combating this issue. What is yet unclear is if their sentiments would translate to any meaningful influence on their product aquisition habbits. They are definitely a better group to target with promotions that highlights the sustainability and environmenmtal friendliness of the products. To be safe though, this message should be embedded in other qualities of the product so that the environmental friendliness would be the Icing on the cake. It would be great for them if they have a product which is good already but also is sustainably produced. \n",
    "\n",
    "\n",
    "It will be beneficial for companies or organizations to band together and form groups where Some organisations are mentioned in the tweets, many which share the same values and ideals when it comes to protecting the environment, who have a substantial membership and following on social media of individuals who share the same values and ideals. The formation of potential partnerships with these organisations could lead to brand exposure with individuals who in their daily lives make conscious decisions with regards to the products and services they purchase.\n",
    "\n",
    "We recommend that the latter strategy of pursuing partnerships with like minded organisations will yield the best results, in terms of finding a group of potential customers who share the same values and ideals, and would be likely to purchase your products and services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"five\"></a>\n",
    "# Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our final model, we build a stacking classifier to combine Logistic Regression, LinearSVC and Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [\n",
    "       ('rf', Pipeline([('Count',CountVectorizer(ngram_range=(1,2))),('classify',RandomForestClassifier())])),\n",
    "         \n",
    "        ('lnsvc', Pipeline([('Count',CountVectorizer(ngram_range=(1,2))),('classify',LinearSVC(C=0.1))])),\n",
    "         \n",
    "        ('MNB',Pipeline([('Count',CountVectorizer()),('classify',MultinomialNB())])),\n",
    "    \n",
    "        ('lr', Pipeline([('Count',CountVectorizer(ngram_range=(1,2))),('classify',LogisticRegression(C=1))]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = StackingClassifier(\n",
    "        estimators=estimators\n",
    "    )\n",
    "\n",
    "#fitting the model\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End experiment\n",
    "experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results on comet page\n",
    "experiment.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the unseen set, so that we can post to Kaggle and recieve a score based on the performance\n",
    "x_unseen = test['processed']\n",
    "\n",
    "submission = pd.DataFrame(\n",
    "    {'tweetid': test['tweetid'],\n",
    "     'sentiment': clf.predict(x_unseen)\n",
    "    })\n",
    "\n",
    "# save DataFrame to csv file for submission\n",
    "submission.to_csv(\"Submission_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\"color: white, background:#000099\">\n",
    "    <b>Copyright &copy 2022 Datafluent Inc.</b>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
