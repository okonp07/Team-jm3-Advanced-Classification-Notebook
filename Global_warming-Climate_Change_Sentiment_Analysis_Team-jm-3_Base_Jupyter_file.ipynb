{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\"color: #000099\"> <h1><b>Climate Change Belief Analysis 2022</b></h1>\n",
    "    <h4> By </h4>\n",
    "    <h2>Datafluent Inc.</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Datafluent Inc.\n",
    "In this era of `Information Technology (IT)`, the variety and quantity of data generated on a daily basis is unprecedented. Data, which is sometimes referred to as the new oil, holds invaluable insights that can make the world a better place and businesses with growth mindsets have started exploiting this invaluable resource to keep thriving in the business world.\n",
    "\n",
    "Behold **`Datafluent Inc.!`** We are an organisation that enjoys squeezing out every bit of actionable insights from a variety of raw facts and communicating such insights in digestible chunks to our stakeholders. We are a team of five seasoned professionals that excelled in various disciplines. \n",
    "\n",
    "## Meet the Team\n",
    "\n",
    "<div align=\"center\" style=\"width: 900px; font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"The team.jpg\"\n",
    "     alt=\"The team\"\n",
    "     style=\"float: center; padding-bottom=0.5em\"\n",
    "     width=900px/>\n",
    "</div>\n",
    "\n",
    "## Project Overview\n",
    "Industrialization is the enabler of modern economic growth and development. However, this comes at the cost of emitting greenhouse gases that contribute, negatively, to climate change and ultimately global warming. Governments, eco-conscious organizations, and civil societies around the world are constantly exploring ways to reduce their carbon footprints. Despite many indices pointing towards climate change, many people are of the belief that climate change is a hoax. Datafluent Inc. is a corporate entity that specializes in mining out actionable insights from raw facts. In this project, we are contracted to use the novel tweets of some individuals to build various Machine Learning (ML) models to identify their beliefs about climate change. These models will be evaluated using the metrics: ***`accuracy, precision, recall, and F1-scores`*** to choose the optimal model. The optimal model’s outcome will help our client to predict how their eco-friendly products will be received by their prospective customers and thus enable them to make strategic business decisions.\n",
    "\n",
    "## Table of Contents\n",
    "<a href=#one>1.0 Introduction</a>\n",
    "- <a href=#1.1>1.1 Project Overview</a>\n",
    "- <a href=#1.2>1.2 Installing Dependencies and Importing Packages</a>\n",
    "- <a href=#1.3>1.3 Loading the Data</a>\n",
    "\n",
    "<a href=#two>2.0 Exploratory Data analysis</a>\n",
    "- <a href=#2.1>2.1 Data Wrangling</a>\n",
    "\n",
    "<a href=#three>3.0 Model Building and Evaluations</a>\n",
    "- <a href=#3.1>3.1 Train-test Split</a>\n",
    "- <a href=#3.2>3.2 Feature Selection</a>\n",
    "- <a href=#3.3>3.3 Model Building</a>\n",
    "- <a href=#3.4>3.4 Model Evaluation and Performance</a>\n",
    "- <a href=#3.5>3.5 Parameter Tunings</a>\n",
    "\n",
    "<a href=#four>4.0 Conclusion</a>\n",
    "\n",
    "<a href=#five>5.0 Submission</a>\n",
    "\n",
    "<a href=#six>References</a>\n",
    "\n",
    "<a id=\"one\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"one\"></a>\n",
    "# 1.0 Introduction\n",
    "Climate change is a big issue that needs serious attention and many eco-conscious organisations are exploring ways to build eco-friendly products. However, the populace is divided in their beliefs about climate change. **`Datafluent Inc.`** was hired by one such eco-conscious company to build a solution that determines the proportion of the population that believes in climate change. The information that the solution extracts will serve as a basis for the organisation strategic decision-making in building eco-friendly products. In this section, we are going to set the stage for our project on climate change belief analysis of about 16,000 tweets of some individuals. We will also explore the `Machine Learning (ML) tools` we will use in building the sentiment analysis model.\n",
    "\n",
    "## 1.1 Global Climate Change\n",
    "Global climate change refers to the average long-term change of climatic conditions over the entire earth. These include warming temperatures and changes in the precipitation as well as the effects of earth's warming such as:\n",
    "\n",
    "* rising sea levels\n",
    "* shrinking mountain glaciers\n",
    "* ice melting at a faster rate than usual in Greenland and Artic\n",
    "* wildfires\n",
    "* floods\n",
    "* droughts\n",
    "* famine\n",
    "* increasing desertification etc.\n",
    "\n",
    "There is a consensus in the scientific community that the earth has been getting warmer in the recent past and this has been linked to human activities, particularly the use of fossil fuels. The dangers of climate change spark debate among people, as we will see when exploring the tweets.  \n",
    "\n",
    "Earth’s climate has constantly been changing, even long before humans came into the picture. However, scientists have observed unusual changes recently. For example, Earth’s average temperature has been increasing much more quickly than they would expect over the past 150 years as shown in figure 1.1 below. According to scientists, the main human activities that contribute to climate change over the last century are the burning of fossil fuels like coal and oil as well as carbon pollution, which has increased the concentration of atmospheric carbon dioxide(CO2), which is a known greenhouse gas.\n",
    "<hr>\n",
    "<div align=\"center\" style=\"width: 500px; font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"World_Climate_Change_Chart.png\"\n",
    "     alt=\"Dummy image 1\"\n",
    "     style=\"float: center; padding-bottom=0.5em\"\n",
    "     width=900px/>|\n",
    "    <div align=\"center\">Figure 1.1: World Climate Change Chart</div>\n",
    " </div>\n",
    "\n",
    "## 1.2 Sentiment Analysis\n",
    "Sentiment analysis is a way of using natural language processing (NLP) and ML tools in identifying opinions expressed in texts. These opinions can be classified as positive (scored as 1), negative (scored as -1), or neutral (scored as 0). Sentiment analysis is important to businesses because it can reveal insights that can be used in improving sales, customer retention, timely problem identification, evaluating customers’ product perceptions, and exploring business opportunities.\n",
    "\n",
    "In this regard, people can be classified into three based on their perceptions about the causes of climate change. Thus,\n",
    "1. Pro-sentiment class: they believe that humans contribute to climate change\n",
    "2. Anti-sentiment class: these are those who do not believe in climate change and those that don’t believe that humans contribute to climate change\n",
    "3. Neutral class: these are those who take a neutral stand on the matter, displaying no strong positive or negative sentiment.\n",
    "\n",
    "### *1.2.1 Techniques of sentiment analysis*\n",
    "Sentiment analysis can be realized in two ways: rule-based and ML sentiment analysis. In rule-based analysis, manual NLP rules like lexicons (lists of words), stemming, tokenization, and parsing are used to simplify complex texts for computers to easily decipher opinions. Whereas, in ML sentiment analysis classification techniques like Naive Bayes, Logistic Regression, Support Vector Machines, Linear Regression, and Deep Learning are used to classify the contextual opinions.\n",
    "\n",
    "Quality insights are key to data-driven decision-making. At **`Datafluent Inc.`** we strongly believe in extracting only quality insights. Thus, we are going to analyze the given dataset thoroughly using both rule-based and ML techniques of sentiment analysis. This will ensure that no insight is left unturned in informing the decision-making process of our **`esteemed clients.`**\n",
    "\n",
    "## 1.3 Classification Techniques\n",
    "We are going to explore the dataset using the following models, compare them and choose the best amongst them.\n",
    "- Decision Tree Classifier\n",
    "- Random Forest Classifier\n",
    "- Linear Support Vector Classifier (LinearSVC)\n",
    "- Support Vector Classifier (SVC)\n",
    "- Logistic Regression\n",
    "- Stochastic Gradient Descent (SGD)\n",
    "\n",
    "### *1.3.1 Decision tree classifier*\n",
    "Decision Trees (DTs) are non-parametric supervised learning methods used for classification and regression. Decision trees learn from data to approximate a sine curve with a set of if-then-else decision rules as illustrated in figure 1.2 below. The deeper the tree, the more complex the decision rules, and the fitter the model.\n",
    "\n",
    "<div align=\"center\" style=\"width: 500px; font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"http://www.wordtemplatesdocs.org/wp-content/uploads/2017/11/Decision-Tree-template-free-00004.jpg\"\n",
    "     alt=\"Decision Tree\"\n",
    "     style=\"float: center; padding-bottom=0.5em\"\n",
    "     width=900px/>\n",
    "    <div align=\"center\">Figure 1.2: Dec</div>\n",
    " </div>\n",
    "\n",
    "A decision tree builds classification or regression models in the form of a tree structure. It breaks down data by partitioning it into subsets after each decision while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes. A decision node has two or more branches. A leaf node represents a classification or decision. The topmost decision node in a tree that corresponds to the best predictor is called the root node. Decision trees can handle both categorical and numerical data.\n",
    "Decision trees are prone to overfitting. Overfitting happens when the learning algorithm continues to develop hypotheses that reduce training set error at the cost of an increased test set error; One method to tackle overfitting in decision trees is by pruning. There are several approaches used to avoid overfitting in building decision trees namely,\n",
    "Pre-pruning stops growing the tree earlier, before it perfectly classifies the training set.\n",
    "Post-pruning allows the tree to perfectly classify the training set, and then post-prune the tree. Practically, the second approach of post-pruning overfits trees is more successful because it is not easy to precisely estimate when to stop growing the tree.\n",
    "Decision Trees are building blocks for the next machine learning method we will look into, which is the Random Forest Classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context of the Problem\n",
    "\n",
    "    \n",
    "Sentiment analysis refers to the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information.\n",
    " |\n",
    "![twitter-bird-animated-logo.gif?w=300&zoom=2](https://dropnerblog.files.wordpress.com/2019/12/twitter-bird-animated-logo.gif?w=300&zoom=2)\n",
    "    \n",
    "## Context of the Problem\n",
    "Companies are constructed around lessening one's environmental impact or carbon footprint. They offer products and services that are environmentally friendly and sustainable, in line with their values and ideals. They would like to determine how people perceive climate change and whether or not they believe it is a real threat. Before a company begins to develop products for a segment of the population that believes in climate change and the effects of climate change on the environment and world, it is logical for such a company to first establish the existence of such demography. Even when it is clear that a section of the population indeed believes that there is a threat of environmental change, we must assess such factors as how much of a threat they believe climate change is. How far they may be willing to go to protect the environment and how passionate they are about supporting the efforts of others to fight climate change.\n",
    "\n",
    "In this experiment, we aim to explore machine learning as a method to assist us in identifying whether or not a person believes in climate change and ascertain if such a person could be converted to a new customer based on their tweets. We will develop ML models that are capable of classifying tweets leaning positively towards a belief that Climate change is a problem and hence desirous of proferring a solution to this problem or leaning negatively towards that belief, hence treating climatic change as no threat. To produce these models, we must first select an appropriate set of data for training that has been annotated. Then, we pre-process and clean these data, to enable us to generate a feature and target set from them. We will then select relevant ML models, on which to train these datasets and evaluate their performance. Model evaluation is done by comparing model predictions against data that have been annotated but not used for training. Once a satisfactory performance of the model has been achieved, we interpret the patterns learned and apply them for further decision-making in the context of the experiment.\n",
    "\n",
    "<div align=\"center\" style=\"width: 950px; font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://www.itu.int/en/mediacentre/backgrounders/PublishingImages/climate-change-backgrounder.jpg\"\n",
    "     alt=\"Dummy image 1\"\n",
    "     style=\"float: center; padding-bottom=0.5em\"\n",
    "     width=950px/>\n",
    "    </div>\n",
    "\n",
    "    ### Problem Statement\n",
    "    Create a Natural Language Processing model to classify whether or not a person believes in climate change, based on their novel tweet data.\n",
    "    \n",
    "    ## Comet ML as a tool for Data Science\n",
    "   <a id=\"comet\"></a>\n",
    "# Comet ML as a tool for Data Science\n",
    "<img src=\"https://d15shllkswkct0.cloudfront.net/wp-content/blogs.dir/1/files/2021/11/cometml.png\" width=\"1000\" alt=\"Drawing\" style=\"width: 1000px;\"/>\n",
    "We will be using Comet to facilitate the model building process as well as a form of version control throughout the development of our model. To begin, we will first install the dependency that allows comet ML to integrate with our notebook and log all changes in our ML model as independent experiments in our comet account. This will allow us to have different versions of our model and will be really helpful in assessing the different properties and functionality of our model versions. This process is done in 3 distinct phases, Pip installation, importation of the experiment, and linking our notebook with our comet account. These processes are completed in the next three cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1.12\"></a>\n",
    "## 1.2 Installing Dependencies and Importing Packages\n",
    "In order to successfully build the models, there is a need to pip install some dependencies. Thus, we install the *autotime library, Comet, imblearn, and nltk.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 33.8 s (started: 2022-06-21 11:39:12 +01:00)\n"
     ]
    }
   ],
   "source": [
    "# Installations\n",
    "%%capture\n",
    "!pip install ipython-autotime\n",
    "!pip install comet-ml\n",
    "!pip install imblearn --user\n",
    "!pip install --user -U nltk\n",
    "!pip install wordcloud\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install -c anaconda -c conda-forge -c comet_ml comet_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2022-06-21 11:56:10 +01:00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Buhari\n",
      "[nltk_data]     Shehu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Buhari\n",
      "[nltk_data]     Shehu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Libraries for data loading, data manipulation and data visulisation\n",
    "import numpy as np # for linear algebra\n",
    "import pandas as pd # for importing, creating and manipulating dataframes\n",
    "\n",
    "# Visualization Packages\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Warnings\n",
    "import warnings \n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Import comet_ml \n",
    "from comet_ml import Experiment\n",
    "\n",
    "# Packages for text manipulation and Natural language processing\n",
    "import re\n",
    "from string import punctuation\n",
    "import nltk\n",
    "nltk.download(['stopwords','punkt'])\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# Train-test split package\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "# Libraries for data preparation and model building and evaluations\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from imblearn.pipeline import pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an experiment with your api key\n",
    "experiment = Experiment(\n",
    "    api_key=\"KQ1UTh7hBvPLWlz3034oIgusG\",\n",
    "    project_name=\"global-warming-climate-change-sentiment-analysis-zm3\",\n",
    "    workspace=\"okonp07@-gmail-com\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1.3\"></a>\n",
    "## 1.3 Loading the Data\n",
    "To load your data, first ensure that the raw data and the notebook file are in the same folder on your local machine. The code below will load both the train and test data sets into your notebook. If the files are not in the same folder, you will have to point to the directory in your machine or cloud location where the file is located. After loading your data, it is good practice to call up the loaded data just to verify that the data actually loaded as it should."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 141 ms (started: 2022-06-21 15:09:07 +01:00)\n"
     ]
    }
   ],
   "source": [
    "#Load the train and test data sets from their respective CSV files\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test_with_no_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"two\"></a>\n",
    "# 2.0 Exploratory Data analysis\n",
    "\n",
    "In this section, we are going to objectively analyse the characterisitcs of our train and test data sets, independently, to have a grasp of the features of the data sets.\n",
    "\n",
    " Some useful questions: \n",
    "* What is the sample size?\n",
    "* What key- words are useful to establish sentiments?\n",
    "* What are the sources of the data (News with verifiable sources, Informal tweets, etc )\n",
    "* Establish sentiments and their percentages in the data (Pro, Anti, Neutral, etc)\n",
    "* Check for words most commonly featured in the dataset\n",
    "* Frwequent Hashtags (Pro and anti)\n",
    "* Consider other conditionalities that will generate insiteful Visuals for you data and use them. \n",
    "\n",
    "Remember that EDA is all about visual presentation. Use visuals to tell the Story of your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a glimpse into the datasets and their shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
       "      <td>625221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
       "      <td>126103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @RawStory: Researchers say we have three ye...</td>\n",
       "      <td>698562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n",
       "      <td>573736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n",
       "      <td>466954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  tweetid\n",
       "0          1  PolySciMajor EPA chief doesn't think carbon di...   625221\n",
       "1          1  It's not like we lack evidence of anthropogeni...   126103\n",
       "2          2  RT @RawStory: Researchers say we have three ye...   698562\n",
       "3          1  #TodayinMaker# WIRED : 2016 was a pivotal year...   573736\n",
       "4          1  RT @SoyNovioDeTodas: It's 2016, and a racist, ...   466954"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 407 ms (started: 2022-06-21 14:08:46 +01:00)\n"
     ]
    }
   ],
   "source": [
    "# first five rows of the dataset\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15819, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2022-06-21 14:09:38 +01:00)\n"
     ]
    }
   ],
   "source": [
    "# checking the shape of the dataset\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15819 entries, 0 to 15818\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   sentiment  15819 non-null  int64 \n",
      " 1   message    15819 non-null  object\n",
      " 2   tweetid    15819 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 370.9+ KB\n",
      "time: 782 ms (started: 2022-06-21 14:14:31 +01:00)\n"
     ]
    }
   ],
   "source": [
    "# checking for missing values and data types\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the above that the train dataset consists of 15819 tweets along with their sentiment scores. The dataset is deviod of any missing values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.1\"></a>\n",
    "### 2.1 Data Wrangling\n",
    "It is essential to clean and preprocess the datasets so as to build more accurate models.The following function will clean and preprocess any tweet parsed into it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 15 ms (started: 2022-06-21 14:45:05 +01:00)\n"
     ]
    }
   ],
   "source": [
    "def tweet_preprocessing(tweet):\n",
    "    \n",
    "    '''\n",
    "    This functions cleans tweets from line breaks, URLs, numbers, etc.\n",
    "    '''\n",
    "    \n",
    "    tweet = tweet.lower() #to lower case\n",
    "    tweet = tweet.replace('\\n', ' ') # remove line breaks\n",
    "    tweet = tweet.replace('\\@(\\w*)', '') # remove mentions\n",
    "    tweet = re.sub(r\"\\bhttps://t.co/\\w+\", '', tweet) # remove URLs\n",
    "    tweet = re.sub('\\w*\\d\\w*', '', tweet) # remove numbers\n",
    "    tweet = re.sub(r'\\#', '', tweet) # remove hashtags. To remove full hashtag: '\\#(\\w*)'\n",
    "    tweet = re.sub('\\w*\\d\\w*', '', tweet) # removes numbers?\n",
    "    tweet = re.sub(' +', ' ', tweet) # remove 1+ spaces\n",
    "\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"three\"></a>\n",
    "# 3.0 Model Building and Evaluations\n",
    "In this section, we are going to build and evalaute various ML models and select the best one amongst them. Will attempt improving the performance of our models by hyper parameter tuning and we will evaluate the performance of these in our dataset:\n",
    "- Naive Bayes Classifier\n",
    "- SVC and LinearSVC\n",
    "- Logistic Regression\n",
    "- KNN\n",
    "- Decision Tree\n",
    "- Random Forests\n",
    "\n",
    "<a id=\"3.1\"></a>\n",
    "## 3.1 Train-test Split\n",
    "After we create a function for preprocessing we must split the data into labels and features (X and y) in order to enable us the run the models on our data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.05 s (started: 2022-06-21 14:49:24 +01:00)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the labels and features\n",
    "train['processed'] = train['message'].apply(tweet_preprocessing)\n",
    "X = train['processed'].values\n",
    "y = train['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 641 ms (started: 2022-06-21 14:49:25 +01:00)\n"
     ]
    }
   ],
   "source": [
    "# preprocess testing data by applying our function\n",
    "test['processed'] = test['message'].apply(tweet_preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.2\"></a>\n",
    "## 3.2 Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.3\"></a>\n",
    "## 3.3 Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Naive Bayes Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 218 ms (started: 2022-06-21 14:50:34 +01:00)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the labels and fetures into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10,random_state=42,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = Pipeline([('Count',CountVectorizer()),('classify',MultinomialNB())])\n",
    "#fitting the model\n",
    "mnb.fit(X_train, y_train)\n",
    "\n",
    "#apply model on test data\n",
    "y_pred_mnb = mnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print(classification_report(y_test, y_pred_mnb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the '-1' and '0' class are poorly predicted when using unbalanced data. Once we implement resampling their f1-score increases for these model but only slightly. While at the same time the overall accuracy is slightly reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. SVC and LinearSVC\n",
    "\n",
    "SVC Provides a best fit to catergorize our data this fit can be nonlinear, while a linearSVC provides a linear interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVC\n",
    "svc = Pipeline([('Count',CountVectorizer()),('classify',SVC(max_iter=300,C=1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linearSVC\n",
    "linsvc = Pipeline([('Count',CountVectorizer()),('classify',LinearSVC(max_iter=300,C=1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Logistic Regression\n",
    "\n",
    "Models the discrete probability distribution between classes and classifies based on the inflection point of the curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "lr = Pipeline([('Count',CountVectorizer()),('classify',LogisticRegression(max_iter=300))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. KNN\n",
    "The KNN classifier assumes that all data points that similar data points tend to form clusters, close together. It classifies points that are close into the same class.K is the number of neighbours. So K=3 implies we will make our predictions based off of the 3 closest points to the data point beign assessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the KNN classifier\n",
    "knn = Pipeline([('Count',CountVectorizer()),('classify',KNeighborsClassifier(n_neighbors=3))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e. Decision Tree\n",
    "\n",
    "The decision tree uses a tree-like model of decisions and their possible consequences.Starting from the decision itself (called a \"node\"), each branch of the decision tree represents a possible decision, outcome, or reaction and it works up until there is only one possible outcome left.\n",
    "<div align=\"center\" style=\"width: 500px; font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://www.aihr.com/wp-content/uploads/decision-trees-in-analytics.png\"\n",
    "     alt=\"Dummy image 1\"\n",
    "     style=\"float: center; padding-bottom=0.5em\"\n",
    "     width=500px/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree\n",
    "dt = Pipeline([('Count',CountVectorizer()),('classify',DecisionTreeClassifier())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f. Random Forest\n",
    "Using the decision tree as a base estimator,each estimator is trained on a different bootstrap sample having the same size as the training set. At each node of the forest, features are sampled without replacement to increase randomization. Nodes are split to maximise information gain.   \n",
    "\n",
    "<div align=\"center\" style=\"width: 500px; font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2020/05/rfc_vs_dt11.png\"\n",
    "     alt=\"Dummy image 1\"\n",
    "     style=\"float: center; padding-bottom=0.5em\"\n",
    "     width=500px/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call up the Random Forest Sampler\n",
    "rf = Pipeline([('Count',CountVectorizer()),('classify',RandomForestClassifier())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.4\"></a>\n",
    "### 3.4 Model Evaluation and Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'svc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m num\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# SVC\u001b[39;00m\n\u001b[0;32m      3\u001b[0m scores \u001b[38;5;241m=\u001b[39m cross_val_score(\n\u001b[1;32m----> 4\u001b[0m         \u001b[43msvc\u001b[49m, X, y, cv\u001b[38;5;241m=\u001b[39mnum, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1_weighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe average weighted F1 score over \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(num)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m SVC models is \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28msum\u001b[39m(scores)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(scores)))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'svc' is not defined"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 156 ms (started: 2022-06-21 15:27:09 +01:00)\n"
     ]
    }
   ],
   "source": [
    "num=3\n",
    "# SVC\n",
    "scores = cross_val_score(\n",
    "        svc, X, y, cv=num, scoring='f1_weighted')\n",
    "print('The average weighted F1 score over '+str(num)+' SVC models is ' + str(sum(scores)/len(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linearSVC\n",
    "scores = cross_val_score(\n",
    "        linsvc, X, y, cv=num, scoring='f1_weighted')\n",
    "print('The average weighted F1 score over '+str(num)+ ' LinearSVC models is ' + str(sum(scores)/len(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "scores = cross_val_score(\n",
    "        lr, X, y, cv=num, scoring='f1_weighted')\n",
    "print('The average weighted F1 score over '+str(num)+' Logistic Regression models is ' + str(sum(scores)/len(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN\n",
    "scores = cross_val_score(\n",
    "        knn, X, y, cv=num, scoring='f1_weighted')\n",
    "print('The average weighted F1 score over '+str(num)+' KNN models is ' + str(sum(scores)/len(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree\n",
    "scores = cross_val_score(\n",
    "        dt, X, y, cv=num, scoring='f1_weighted')\n",
    "print('The average weighted F1 score over '+str(num)+' Decision Tree models is ' + str(sum(scores)/len(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "scores = cross_val_score(\n",
    "        rf, X, y, cv=num, scoring='f1_weighted')\n",
    "print('The average weighted F1 score over '+str(num)+' KNN models is ' + str(sum(scores)/len(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Logistic Regression Model and the LinearSVC model perform the best. The best performance for every model is found when resampling is not done. This could be because because upsampling the minority classes to the level of the majority class results in too much overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.5\"></a>\n",
    "### 3.5 Parameter Tunings\n",
    "\n",
    "We take a look and see if we can improve our best 2 models: linearSVC and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "Cs = [0.001, 0.01, 0.1, 1, 10]\n",
    "param_grid = {\n",
    "    'C'     : Cs\n",
    "    }\n",
    "grid_SVM = GridSearchCV(LogisticRegression(), param_grid, scoring='f1_weighted', cv=3)\n",
    "grid_SVM.fit(CountVectorizer().fit_transform(X), y)\n",
    "grid_SVM.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C'     : Cs }\n",
    "grid_SVM = GridSearchCV(LinearSVC(), param_grid, scoring='f1_weighted', cv=3)\n",
    "grid_SVM.fit(CountVectorizer().fit_transform(X), y)\n",
    "grid_SVM.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"four\"></a>\n",
    "# 4.0 Conclusion\n",
    "\n",
    "### Model performance\n",
    "Several strategies we attempted to improve model performance, ranging from data processing techniques to clean the tweets, data balancing strategies, cross validation and grid search for the best values for model hyperparameters.\n",
    "\n",
    "\n",
    "\n",
    "### What else we can try\n",
    "Language models and the use of neural networks were two other strategies that we may implement to see how it will compare with essemble tree based models. It will also provide an opportunity for us to see how neural network models perforn on Natural Language tasks.\n",
    "\n",
    "### Business case value\n",
    "\n",
    "The analysis shows generally that the sentiment from the negative tweets arise from a a class of people who believe that climate change and the incidence of global warming is a hoax, or at best overated. Most of these persons defend their ideology with strong resolve and it will therefore be counter productive to try to market environmentally friendly products to them as an effort towards sustainability or to combat the incidence of Climate change. This class are more likely to become customers if other aspects of the product is promoted to them. They are more likely to purchase a product because of quality, fairness in price, use case etc rather than just because it is sustainably produced or good for the environmenmt.\n",
    "\n",
    "Conversely, People from the other end of the spectrum who display positive sentiments towards climate change definitely believe that climate change is an issue. They show some willingness to \"do something\" to play their role in combating this issue. What is yet unclear is if their sentiments would translate to any meaningful influence on their product aquisition habbits. They are definitely a better group to target with promotions that highlights the sustainability and environmenmtal friendliness of the products. To be safe though, this message should be embedded in other qualities of the product so that the environmental friendliness would be the Icing on the cake. It would be great for them if they have a product which is good already but also is sustainably produced. \n",
    "\n",
    "\n",
    "It will be beneficial for companies or organizations to band together and form groups where Some organisations are mentioned in the tweets, many which share the same values and ideals when it comes to protecting the environment, who have a substantial membership and following on social media of individuals who share the same values and ideals. The formation of potential partnerships with these organisations could lead to brand exposure with individuals who in their daily lives make conscious decisions with regards to the products and services they purchase.\n",
    "\n",
    "We recommend that the latter strategy of pursuing partnerships with like minded organisations will yield the best results, in terms of finding a group of potential customers who share the same values and ideals, and would be likely to purchase your products and services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"five\"></a>\n",
    "# Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our final model, we build a stacking classifier to combine Logistic Regression, LinearSVC and Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [\n",
    "       ('rf', Pipeline([('Count',CountVectorizer(ngram_range=(1,2))),('classify',RandomForestClassifier())])),\n",
    "         \n",
    "        ('lnsvc', Pipeline([('Count',CountVectorizer(ngram_range=(1,2))),('classify',LinearSVC(C=0.1))])),\n",
    "         \n",
    "        ('MNB',Pipeline([('Count',CountVectorizer()),('classify',MultinomialNB())])),\n",
    "    \n",
    "        ('lr', Pipeline([('Count',CountVectorizer(ngram_range=(1,2))),('classify',LogisticRegression(C=1))]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = StackingClassifier(\n",
    "        estimators=estimators\n",
    "    )\n",
    "\n",
    "#fitting the model\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End experiment\n",
    "experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results on comet page\n",
    "experiment.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the unseen set, so that we can post to Kaggle and recieve a score based on the performance\n",
    "x_unseen = test['processed']\n",
    "\n",
    "submission = pd.DataFrame(\n",
    "    {'tweetid': test['tweetid'],\n",
    "     'sentiment': clf.predict(x_unseen)\n",
    "    })\n",
    "\n",
    "# save DataFrame to csv file for submission\n",
    "submission.to_csv(\"Submission_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\"color: white, background:#000099\">\n",
    "    <b>Copyright &copy 2022 Datafluent Inc.</b>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
